{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install pytorch-lightning transformers wandb scikit-learn pandas numpy torch h5py --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from sklearn.utils import shuffle\n",
    "from typing import List, Tuple\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import pytorch_lightning as pl\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "def split_and_save_data(train_df: pd.DataFrame, \n",
    "                       val_df: pd.DataFrame,\n",
    "                       train_chunk_size: int = 500000,\n",
    "                       val_chunk_size: int = 100000,\n",
    "                       output_dir: str = 'data_chunks') -> Tuple[List[str], List[str]]:\n",
    "    \"\"\"\n",
    "    Split and save training and validation data into smaller chunks\n",
    "    \"\"\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    # Shuffle the dataframes\n",
    "    train_df = shuffle(train_df, random_state=42)\n",
    "    val_df = shuffle(val_df, random_state=42)\n",
    "    \n",
    "    # Split and save training data\n",
    "    train_chunks = []\n",
    "    for i in range(0, len(train_df), train_chunk_size):\n",
    "        chunk = train_df.iloc[i:i + train_chunk_size]\n",
    "        filename = f'{output_dir}/train_chunk_{i//train_chunk_size}.csv'\n",
    "        chunk.to_csv(filename, index=False)\n",
    "        train_chunks.append(filename)\n",
    "    \n",
    "    # Split and save validation data\n",
    "    val_chunks = []\n",
    "    for i in range(0, len(val_df), val_chunk_size):\n",
    "        chunk = val_df.iloc[i:i + val_chunk_size]\n",
    "        filename = f'{output_dir}/val_chunk_{i//val_chunk_size}.csv'\n",
    "        chunk.to_csv(filename, index=False)\n",
    "        val_chunks.append(filename)\n",
    "    \n",
    "    return train_chunks, val_chunks\n",
    "\n",
    "def train_model_incrementally(train_chunks: List[str],\n",
    "                            val_chunks: List[str],\n",
    "                            model_dir: str = 'models',\n",
    "                            batch_size: int = 32,\n",
    "                            epochs_per_chunk: int = 10):\n",
    "    \"\"\"\n",
    "    Train the model incrementally on data chunks\n",
    "    \"\"\"\n",
    "    os.makedirs(model_dir, exist_ok=True)\n",
    "    tokenizer = AutoTokenizer.from_pretrained('BAAI/bge-small-en-v1.5')\n",
    "    \n",
    "    model = None\n",
    "    label_encoder = None\n",
    "    \n",
    "    for chunk_idx, (train_chunk, val_chunk) in enumerate(zip(train_chunks, val_chunks)):\n",
    "        print(f\"\\nTraining on chunk {chunk_idx + 1}/{len(train_chunks)}\")\n",
    "        \n",
    "        # Load data chunks\n",
    "        train_df = pd.read_csv(train_chunk)\n",
    "        val_df = pd.read_csv(val_chunk)\n",
    "        \n",
    "        # Create datasets\n",
    "        if model is None:  # First chunk\n",
    "            train_dataset = AccidentDataset(train_df, tokenizer)\n",
    "            val_dataset = AccidentDataset(val_df, tokenizer,\n",
    "                                        text_embeddings=train_dataset.get_text_embeddings(val_df['Description'].tolist()))\n",
    "            \n",
    "            # Initialize model\n",
    "            model = AccidentPredictor(\n",
    "                tabular_dim=train_dataset.tabular_features.shape[1],\n",
    "                embedding_dim=train_dataset.text_embeddings.shape[1],\n",
    "                num_classes=len(train_dataset.label_encoder.classes_)\n",
    "            )\n",
    "            label_encoder = train_dataset.label_encoder\n",
    "        else:  # Subsequent chunks\n",
    "            train_dataset = AccidentDataset(train_df, tokenizer, label_encoder=label_encoder)\n",
    "            val_dataset = AccidentDataset(val_df, tokenizer,\n",
    "                                        text_embeddings=train_dataset.get_text_embeddings(val_df['Description'].tolist()),\n",
    "                                        label_encoder=label_encoder)\n",
    "        \n",
    "        # Create dataloaders\n",
    "        train_loader = DataLoader(train_dataset, \n",
    "                                batch_size=batch_size, \n",
    "                                shuffle=True, \n",
    "                                num_workers=4,\n",
    "                                pin_memory=True)\n",
    "        val_loader = DataLoader(val_dataset, \n",
    "                              batch_size=batch_size, \n",
    "                              shuffle=False, \n",
    "                              num_workers=4,\n",
    "                              pin_memory=True)\n",
    "        \n",
    "        # Initialize wandb logger with unique name for each chunk\n",
    "        wandb_logger = WandbLogger(project='accident-severity-prediction',\n",
    "                                 name=f'hybrid-model-chunk-{chunk_idx}')\n",
    "        \n",
    "        # Initialize trainer\n",
    "        trainer = pl.Trainer(\n",
    "            max_epochs=epochs_per_chunk,\n",
    "            logger=wandb_logger,\n",
    "            callbacks=[\n",
    "                ModelCheckpoint(\n",
    "                    dirpath=f'{model_dir}/chunk_{chunk_idx}',\n",
    "                    filename='accident-predictor-{epoch:02d}-{val_loss:.2f}',\n",
    "                    save_top_k=1,\n",
    "                    mode='min'\n",
    "                ),\n",
    "                EarlyStopping(monitor='val_loss', patience=3, mode='min'),\n",
    "                LitProgressBar()\n",
    "            ],\n",
    "            accelerator='auto',\n",
    "            devices=1,\n",
    "            log_every_n_steps=10\n",
    "        )\n",
    "        \n",
    "        # Train model\n",
    "        trainer.fit(model, train_loader, val_loader)\n",
    "        \n",
    "        # Save model after each chunk\n",
    "        torch.save({\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'label_encoder': label_encoder\n",
    "        }, f'{model_dir}/model_after_chunk_{chunk_idx}.pt')\n",
    "        \n",
    "        # Clear memory\n",
    "        del train_dataset, val_dataset, train_loader, val_loader\n",
    "        torch.cuda.empty_cache()\n",
    "    \n",
    "    return model, label_encoder\n",
    "\n",
    "# Modified AccidentDataset class\n",
    "class AccidentDataset(Dataset):\n",
    "    def __init__(self, df: pd.DataFrame, tokenizer, text_embeddings=None, \n",
    "                 is_test: bool = False, label_encoder=None):\n",
    "        self.df = df\n",
    "        self.tokenizer = tokenizer\n",
    "        self.is_test = is_test\n",
    "        \n",
    "        # Process tabular features\n",
    "        print(\"Processing tabular features...\")\n",
    "        self.process_tabular_features()\n",
    "        \n",
    "        # Store or compute text embeddings\n",
    "        if text_embeddings is not None:\n",
    "            self.text_embeddings = text_embeddings\n",
    "        else:\n",
    "            print(\"Computing text embeddings...\")\n",
    "            self.text_embeddings = self.get_text_embeddings(df['Description'].tolist())\n",
    "        \n",
    "        if not is_test:\n",
    "            if label_encoder is None:\n",
    "                self.label_encoder = LabelEncoder()\n",
    "                self.labels = self.label_encoder.fit_transform(df['Severity'])\n",
    "            else:\n",
    "                self.label_encoder = label_encoder\n",
    "                self.labels = self.label_encoder.transform(df['Severity'])\n",
    "\n",
    "\n",
    "# Load your full data\n",
    "train_df = pd.read_csv('train.csv')  # (5796296, 34)\n",
    "val_df = pd.read_csv('val.csv')      # (1159259, 34)\n",
    "\n",
    "# Split data into chunks and save\n",
    "train_chunks, val_chunks = split_and_save_data(\n",
    "    train_df, \n",
    "    val_df,\n",
    "    train_chunk_size=500000,\n",
    "    val_chunk_size=100000,\n",
    "    output_dir='data_chunks'\n",
    ")\n",
    "\n",
    "# Train model incrementally\n",
    "model, label_encoder = train_model_incrementally(\n",
    "    train_chunks,\n",
    "    val_chunks,\n",
    "    model_dir='models',\n",
    "    batch_size=32,\n",
    "    epochs_per_chunk=10\n",
    ")\n",
    "\n",
    "# Save final model\n",
    "torch.save({\n",
    "    'model_state_dict': model.state_dict(),\n",
    "    'label_encoder': label_encoder\n",
    "}, 'models/final_model.pt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating total rows...\n",
      "Creating HDF5 file with shape (5800000, 384)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "475ae21cd8f04993b3fbeb5d4516469c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing text embeddings: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 8\u001b[0m\n\u001b[1;32m      5\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m AutoTokenizer\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mBAAI/bge-small-en-v1.5\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# Process embeddings for train and validation sets\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m \u001b[43mprocess_text_embeddings\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m/teamspace/studios/this_studio/Assignment-TechstaX/data/imputed_dataset.csv\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m/teamspace/studios/this_studio/Assignment-TechstaX/data/train_embeddings.h5\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m    \u001b[49m\u001b[43mchunk_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Adjust based on your GPU memory\u001b[39;49;00m\n\u001b[1;32m     13\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1000\u001b[39;49m\u001b[43m     \u001b[49m\u001b[38;5;66;43;03m# Adjust based on your GPU memory\u001b[39;49;00m\n\u001b[1;32m     14\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     16\u001b[0m process_text_embeddings(\n\u001b[1;32m     17\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/teamspace/studios/this_studio/Assignment-TechstaX/data/val_data.csv\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m     18\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/teamspace/studios/this_studio/Assignment-TechstaX/data/val_embeddings.h5\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     21\u001b[0m     batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1000\u001b[39m\n\u001b[1;32m     22\u001b[0m )\n\u001b[1;32m     23\u001b[0m \u001b[38;5;66;03m# Train model\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[2], line 232\u001b[0m, in \u001b[0;36mprocess_text_embeddings\u001b[0;34m(df_path, output_path, tokenizer, chunk_size, batch_size)\u001b[0m\n\u001b[1;32m    230\u001b[0m     embeddings \u001b[38;5;241m=\u001b[39m output[\u001b[38;5;241m0\u001b[39m][:, \u001b[38;5;241m0\u001b[39m]  \u001b[38;5;66;03m# CLS token\u001b[39;00m\n\u001b[1;32m    231\u001b[0m     embeddings \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mfunctional\u001b[38;5;241m.\u001b[39mnormalize(embeddings, p\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m--> 232\u001b[0m     chunk_embeddings\u001b[38;5;241m.\u001b[39mappend(\u001b[43membeddings\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcpu\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mnumpy())\n\u001b[1;32m    234\u001b[0m \u001b[38;5;66;03m# Clear GPU memory\u001b[39;00m\n\u001b[1;32m    235\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m encoded_input\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "# Initialize tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained('BAAI/bge-small-en-v1.5')\n",
    "\n",
    "# Process embeddings for train and validation sets\n",
    "process_text_embeddings(\n",
    "    '/teamspace/studios/this_studio/Assignment-TechstaX/data/imputed_dataset.csv',\n",
    "    '/teamspace/studios/this_studio/Assignment-TechstaX/data/train_embeddings.h5',\n",
    "    tokenizer,\n",
    "    chunk_size=10000,  # Adjust based on your GPU memory\n",
    "    batch_size=1000     # Adjust based on your GPU memory\n",
    ")\n",
    "\n",
    "process_text_embeddings(\n",
    "    '/teamspace/studios/this_studio/Assignment-TechstaX/data/val_data.csv',\n",
    "    '/teamspace/studios/this_studio/Assignment-TechstaX/data/val_embeddings.h5',\n",
    "    tokenizer,\n",
    "    chunk_size=10000,\n",
    "    batch_size=1000\n",
    ")\n",
    "# Train model\n",
    "model = train_model('/teamspace/studios/this_studio/Assignment-TechstaX/data/imputed_dataset.csv', '/teamspace/studios/this_studio/Assignment-TechstaX/data/val_data.csv', \n",
    "                   '/teamspace/studios/this_studio/Assignment-TechstaX/data/train_embeddings.h5', '/teamspace/studios/this_studio/Assignment-TechstaX/data/val_embeddings.h5')\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
