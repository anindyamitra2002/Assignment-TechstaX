{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install fireducks seaborn torch --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fireducks.pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Source</th>\n",
       "      <th>Severity</th>\n",
       "      <th>Start_Time</th>\n",
       "      <th>End_Time</th>\n",
       "      <th>Start_Lat</th>\n",
       "      <th>Start_Lng</th>\n",
       "      <th>End_Lat</th>\n",
       "      <th>End_Lng</th>\n",
       "      <th>Distance(mi)</th>\n",
       "      <th>...</th>\n",
       "      <th>Roundabout</th>\n",
       "      <th>Station</th>\n",
       "      <th>Stop</th>\n",
       "      <th>Traffic_Calming</th>\n",
       "      <th>Traffic_Signal</th>\n",
       "      <th>Turning_Loop</th>\n",
       "      <th>Sunrise_Sunset</th>\n",
       "      <th>Civil_Twilight</th>\n",
       "      <th>Nautical_Twilight</th>\n",
       "      <th>Astronomical_Twilight</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5855182</th>\n",
       "      <td>A-5899011</td>\n",
       "      <td>Source1</td>\n",
       "      <td>2</td>\n",
       "      <td>2021-12-15 12:49:00</td>\n",
       "      <td>2021-12-15 16:38:37</td>\n",
       "      <td>40.901413</td>\n",
       "      <td>-122.383288</td>\n",
       "      <td>40.889425</td>\n",
       "      <td>-122.383697</td>\n",
       "      <td>0.829</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>Day</td>\n",
       "      <td>Day</td>\n",
       "      <td>Day</td>\n",
       "      <td>Day</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5223405</th>\n",
       "      <td>A-5263033</td>\n",
       "      <td>Source1</td>\n",
       "      <td>2</td>\n",
       "      <td>2023-01-30 11:34:00</td>\n",
       "      <td>2023-01-30 12:07:30</td>\n",
       "      <td>43.235819</td>\n",
       "      <td>-83.769191</td>\n",
       "      <td>43.24594</td>\n",
       "      <td>-83.771264</td>\n",
       "      <td>0.707</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>Day</td>\n",
       "      <td>Day</td>\n",
       "      <td>Day</td>\n",
       "      <td>Day</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6178517</th>\n",
       "      <td>A-6224194</td>\n",
       "      <td>Source1</td>\n",
       "      <td>2</td>\n",
       "      <td>2021-10-15 19:11:00</td>\n",
       "      <td>2021-10-15 19:31:30</td>\n",
       "      <td>34.029034</td>\n",
       "      <td>-118.202791</td>\n",
       "      <td>34.028473</td>\n",
       "      <td>-118.209078</td>\n",
       "      <td>0.362</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>Night</td>\n",
       "      <td>Night</td>\n",
       "      <td>Day</td>\n",
       "      <td>Day</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>284183</th>\n",
       "      <td>A-284191</td>\n",
       "      <td>Source2</td>\n",
       "      <td>2</td>\n",
       "      <td>2016-09-13 13:11:57</td>\n",
       "      <td>2016-09-13 13:41:57</td>\n",
       "      <td>30.345142</td>\n",
       "      <td>-97.714386</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>0.000</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>Day</td>\n",
       "      <td>Day</td>\n",
       "      <td>Day</td>\n",
       "      <td>Day</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5485910</th>\n",
       "      <td>A-5527521</td>\n",
       "      <td>Source1</td>\n",
       "      <td>2</td>\n",
       "      <td>2021-11-16 14:11:30.000000000</td>\n",
       "      <td>2021-11-16 17:49:30.000000000</td>\n",
       "      <td>25.848453</td>\n",
       "      <td>-80.322226</td>\n",
       "      <td>25.872619</td>\n",
       "      <td>-80.32298</td>\n",
       "      <td>1.670</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>Day</td>\n",
       "      <td>Day</td>\n",
       "      <td>Day</td>\n",
       "      <td>Day</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>849296</th>\n",
       "      <td>A-859010</td>\n",
       "      <td>Source2</td>\n",
       "      <td>2</td>\n",
       "      <td>2021-10-15 17:34:10</td>\n",
       "      <td>2021-10-15 18:03:57</td>\n",
       "      <td>38.000614</td>\n",
       "      <td>-122.054100</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>0.000</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>Day</td>\n",
       "      <td>Day</td>\n",
       "      <td>Day</td>\n",
       "      <td>Day</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>210871</th>\n",
       "      <td>A-210878</td>\n",
       "      <td>Source2</td>\n",
       "      <td>2</td>\n",
       "      <td>2016-10-07 15:28:24</td>\n",
       "      <td>2016-10-07 16:26:00</td>\n",
       "      <td>40.959007</td>\n",
       "      <td>-72.727646</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>0.000</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>Day</td>\n",
       "      <td>Day</td>\n",
       "      <td>Day</td>\n",
       "      <td>Day</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2330032</th>\n",
       "      <td>A-2339900</td>\n",
       "      <td>Source2</td>\n",
       "      <td>3</td>\n",
       "      <td>2019-01-16 10:59:12</td>\n",
       "      <td>2019-01-16 11:28:58</td>\n",
       "      <td>37.671970</td>\n",
       "      <td>-77.648987</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>0.000</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>Day</td>\n",
       "      <td>Day</td>\n",
       "      <td>Day</td>\n",
       "      <td>Day</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4056539</th>\n",
       "      <td>A-4087044</td>\n",
       "      <td>Source1</td>\n",
       "      <td>2</td>\n",
       "      <td>2023-01-19 16:36:41</td>\n",
       "      <td>2023-01-19 17:54:42</td>\n",
       "      <td>38.926260</td>\n",
       "      <td>-77.013377</td>\n",
       "      <td>38.926344</td>\n",
       "      <td>-77.014421</td>\n",
       "      <td>0.056</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>Day</td>\n",
       "      <td>Day</td>\n",
       "      <td>Day</td>\n",
       "      <td>Day</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6390133</th>\n",
       "      <td>A-6437261</td>\n",
       "      <td>Source1</td>\n",
       "      <td>2</td>\n",
       "      <td>2021-02-16 12:23:28</td>\n",
       "      <td>2021-02-16 14:23:40</td>\n",
       "      <td>32.290356</td>\n",
       "      <td>-80.917715</td>\n",
       "      <td>32.290787</td>\n",
       "      <td>-80.917787</td>\n",
       "      <td>0.030</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>Day</td>\n",
       "      <td>Day</td>\n",
       "      <td>Day</td>\n",
       "      <td>Day</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 46 columns</p>\n",
       "</div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data = pd.read_csv(\"/teamspace/studios/this_studio/Assignment-TechstaX/data/US_Accidents_March23.csv\")\n",
    "data.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7728394, 46)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of null values in each column:\n",
      "\n",
      "ID                             0\n",
      "Source                         0\n",
      "Severity                       0\n",
      "Start_Time                     0\n",
      "End_Time                       0\n",
      "Start_Lat                      0\n",
      "Start_Lng                      0\n",
      "End_Lat                  3402762\n",
      "End_Lng                  3402762\n",
      "Distance(mi)                   0\n",
      "Description                    5\n",
      "Street                     10869\n",
      "City                         253\n",
      "County                         0\n",
      "State                          0\n",
      "Zipcode                     1915\n",
      "Country                        0\n",
      "Timezone                    7808\n",
      "Airport_Code               22635\n",
      "Weather_Timestamp         120228\n",
      "Temperature(F)            163853\n",
      "Wind_Chill(F)            1999019\n",
      "Humidity(%)               174144\n",
      "Pressure(in)              140679\n",
      "Visibility(mi)            177098\n",
      "Wind_Direction            175206\n",
      "Wind_Speed(mph)           571233\n",
      "Precipitation(in)        2203586\n",
      "Weather_Condition         173459\n",
      "Amenity                        0\n",
      "Bump                           0\n",
      "Crossing                       0\n",
      "Give_Way                       0\n",
      "Junction                       0\n",
      "No_Exit                        0\n",
      "Railway                        0\n",
      "Roundabout                     0\n",
      "Station                        0\n",
      "Stop                           0\n",
      "Traffic_Calming                0\n",
      "Traffic_Signal                 0\n",
      "Turning_Loop                   0\n",
      "Sunrise_Sunset             23246\n",
      "Civil_Twilight             23246\n",
      "Nautical_Twilight          23246\n",
      "Astronomical_Twilight      23246\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Check the number of null values in each column\n",
    "null_values = data.isnull().sum()\n",
    "print(\"Number of null values in each column:\\n\")\n",
    "print(null_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Number of rows with no null values: 3554549\n"
     ]
    }
   ],
   "source": [
    "# Count the number of rows with no null values\n",
    "rows_with_no_nulls = data.dropna().shape[0]\n",
    "print(\"\\nNumber of rows with no null values:\", rows_with_no_nulls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique cities: 1\n",
      "Unique states: 49\n"
     ]
    }
   ],
   "source": [
    "unique_city = len(data[['Country']].value_counts())\n",
    "unique_state = len(data[['State']].value_counts())\n",
    "\n",
    "print(\"Unique cities:\", unique_city)\n",
    "print(\"Unique states:\", unique_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique values and frequencies for column 'ID' saved to /teamspace/studios/this_studio/Assignment-TechstaX/data/unique_value_frequencies/ID_frequencies.csv\n",
      "Unique values and frequencies for column 'Source' saved to /teamspace/studios/this_studio/Assignment-TechstaX/data/unique_value_frequencies/Source_frequencies.csv\n",
      "Unique values and frequencies for column 'Severity' saved to /teamspace/studios/this_studio/Assignment-TechstaX/data/unique_value_frequencies/Severity_frequencies.csv\n",
      "Unique values and frequencies for column 'Start_Time' saved to /teamspace/studios/this_studio/Assignment-TechstaX/data/unique_value_frequencies/Start_Time_frequencies.csv\n",
      "Unique values and frequencies for column 'End_Time' saved to /teamspace/studios/this_studio/Assignment-TechstaX/data/unique_value_frequencies/End_Time_frequencies.csv\n",
      "Unique values and frequencies for column 'Start_Lat' saved to /teamspace/studios/this_studio/Assignment-TechstaX/data/unique_value_frequencies/Start_Lat_frequencies.csv\n",
      "Unique values and frequencies for column 'Start_Lng' saved to /teamspace/studios/this_studio/Assignment-TechstaX/data/unique_value_frequencies/Start_Lng_frequencies.csv\n",
      "Unique values and frequencies for column 'End_Lat' saved to /teamspace/studios/this_studio/Assignment-TechstaX/data/unique_value_frequencies/End_Lat_frequencies.csv\n",
      "Unique values and frequencies for column 'End_Lng' saved to /teamspace/studios/this_studio/Assignment-TechstaX/data/unique_value_frequencies/End_Lng_frequencies.csv\n",
      "Unique values and frequencies for column 'Distance(mi)' saved to /teamspace/studios/this_studio/Assignment-TechstaX/data/unique_value_frequencies/Distance(mi)_frequencies.csv\n",
      "Unique values and frequencies for column 'Description' saved to /teamspace/studios/this_studio/Assignment-TechstaX/data/unique_value_frequencies/Description_frequencies.csv\n",
      "Unique values and frequencies for column 'Street' saved to /teamspace/studios/this_studio/Assignment-TechstaX/data/unique_value_frequencies/Street_frequencies.csv\n",
      "Unique values and frequencies for column 'City' saved to /teamspace/studios/this_studio/Assignment-TechstaX/data/unique_value_frequencies/City_frequencies.csv\n",
      "Unique values and frequencies for column 'County' saved to /teamspace/studios/this_studio/Assignment-TechstaX/data/unique_value_frequencies/County_frequencies.csv\n",
      "Unique values and frequencies for column 'State' saved to /teamspace/studios/this_studio/Assignment-TechstaX/data/unique_value_frequencies/State_frequencies.csv\n",
      "Unique values and frequencies for column 'Zipcode' saved to /teamspace/studios/this_studio/Assignment-TechstaX/data/unique_value_frequencies/Zipcode_frequencies.csv\n",
      "Unique values and frequencies for column 'Country' saved to /teamspace/studios/this_studio/Assignment-TechstaX/data/unique_value_frequencies/Country_frequencies.csv\n",
      "Unique values and frequencies for column 'Timezone' saved to /teamspace/studios/this_studio/Assignment-TechstaX/data/unique_value_frequencies/Timezone_frequencies.csv\n",
      "Unique values and frequencies for column 'Airport_Code' saved to /teamspace/studios/this_studio/Assignment-TechstaX/data/unique_value_frequencies/Airport_Code_frequencies.csv\n",
      "Unique values and frequencies for column 'Weather_Timestamp' saved to /teamspace/studios/this_studio/Assignment-TechstaX/data/unique_value_frequencies/Weather_Timestamp_frequencies.csv\n",
      "Unique values and frequencies for column 'Temperature(F)' saved to /teamspace/studios/this_studio/Assignment-TechstaX/data/unique_value_frequencies/Temperature(F)_frequencies.csv\n",
      "Unique values and frequencies for column 'Wind_Chill(F)' saved to /teamspace/studios/this_studio/Assignment-TechstaX/data/unique_value_frequencies/Wind_Chill(F)_frequencies.csv\n",
      "Unique values and frequencies for column 'Humidity(%)' saved to /teamspace/studios/this_studio/Assignment-TechstaX/data/unique_value_frequencies/Humidity(%)_frequencies.csv\n",
      "Unique values and frequencies for column 'Pressure(in)' saved to /teamspace/studios/this_studio/Assignment-TechstaX/data/unique_value_frequencies/Pressure(in)_frequencies.csv\n",
      "Unique values and frequencies for column 'Visibility(mi)' saved to /teamspace/studios/this_studio/Assignment-TechstaX/data/unique_value_frequencies/Visibility(mi)_frequencies.csv\n",
      "Unique values and frequencies for column 'Wind_Direction' saved to /teamspace/studios/this_studio/Assignment-TechstaX/data/unique_value_frequencies/Wind_Direction_frequencies.csv\n",
      "Unique values and frequencies for column 'Wind_Speed(mph)' saved to /teamspace/studios/this_studio/Assignment-TechstaX/data/unique_value_frequencies/Wind_Speed(mph)_frequencies.csv\n",
      "Unique values and frequencies for column 'Precipitation(in)' saved to /teamspace/studios/this_studio/Assignment-TechstaX/data/unique_value_frequencies/Precipitation(in)_frequencies.csv\n",
      "Unique values and frequencies for column 'Weather_Condition' saved to /teamspace/studios/this_studio/Assignment-TechstaX/data/unique_value_frequencies/Weather_Condition_frequencies.csv\n",
      "Unique values and frequencies for column 'Amenity' saved to /teamspace/studios/this_studio/Assignment-TechstaX/data/unique_value_frequencies/Amenity_frequencies.csv\n",
      "Unique values and frequencies for column 'Bump' saved to /teamspace/studios/this_studio/Assignment-TechstaX/data/unique_value_frequencies/Bump_frequencies.csv\n",
      "Unique values and frequencies for column 'Crossing' saved to /teamspace/studios/this_studio/Assignment-TechstaX/data/unique_value_frequencies/Crossing_frequencies.csv\n",
      "Unique values and frequencies for column 'Give_Way' saved to /teamspace/studios/this_studio/Assignment-TechstaX/data/unique_value_frequencies/Give_Way_frequencies.csv\n",
      "Unique values and frequencies for column 'Junction' saved to /teamspace/studios/this_studio/Assignment-TechstaX/data/unique_value_frequencies/Junction_frequencies.csv\n",
      "Unique values and frequencies for column 'No_Exit' saved to /teamspace/studios/this_studio/Assignment-TechstaX/data/unique_value_frequencies/No_Exit_frequencies.csv\n",
      "Unique values and frequencies for column 'Railway' saved to /teamspace/studios/this_studio/Assignment-TechstaX/data/unique_value_frequencies/Railway_frequencies.csv\n",
      "Unique values and frequencies for column 'Roundabout' saved to /teamspace/studios/this_studio/Assignment-TechstaX/data/unique_value_frequencies/Roundabout_frequencies.csv\n",
      "Unique values and frequencies for column 'Station' saved to /teamspace/studios/this_studio/Assignment-TechstaX/data/unique_value_frequencies/Station_frequencies.csv\n",
      "Unique values and frequencies for column 'Stop' saved to /teamspace/studios/this_studio/Assignment-TechstaX/data/unique_value_frequencies/Stop_frequencies.csv\n",
      "Unique values and frequencies for column 'Traffic_Calming' saved to /teamspace/studios/this_studio/Assignment-TechstaX/data/unique_value_frequencies/Traffic_Calming_frequencies.csv\n",
      "Unique values and frequencies for column 'Traffic_Signal' saved to /teamspace/studios/this_studio/Assignment-TechstaX/data/unique_value_frequencies/Traffic_Signal_frequencies.csv\n",
      "Unique values and frequencies for column 'Turning_Loop' saved to /teamspace/studios/this_studio/Assignment-TechstaX/data/unique_value_frequencies/Turning_Loop_frequencies.csv\n",
      "Unique values and frequencies for column 'Sunrise_Sunset' saved to /teamspace/studios/this_studio/Assignment-TechstaX/data/unique_value_frequencies/Sunrise_Sunset_frequencies.csv\n",
      "Unique values and frequencies for column 'Civil_Twilight' saved to /teamspace/studios/this_studio/Assignment-TechstaX/data/unique_value_frequencies/Civil_Twilight_frequencies.csv\n",
      "Unique values and frequencies for column 'Nautical_Twilight' saved to /teamspace/studios/this_studio/Assignment-TechstaX/data/unique_value_frequencies/Nautical_Twilight_frequencies.csv\n",
      "Unique values and frequencies for column 'Astronomical_Twilight' saved to /teamspace/studios/this_studio/Assignment-TechstaX/data/unique_value_frequencies/Astronomical_Twilight_frequencies.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "output_dir = '/teamspace/studios/this_studio/Assignment-TechstaX/data/unique_value_frequencies/'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Iterate through each column and calculate unique values and their frequencies\n",
    "for column in data.columns:\n",
    "    unique_counts = data[column].value_counts(dropna=False)  # Include NaN values\n",
    "    output_df = unique_counts.reset_index()  # Convert to DataFrame\n",
    "    output_df.columns = ['Unique_Value', 'Frequency']  # Rename columns\n",
    "    \n",
    "    # Save the unique value frequencies to a CSV file\n",
    "    output_file = os.path.join(output_dir, f\"{column}_frequencies.csv\")\n",
    "    output_df.to_csv(output_file, index=False)\n",
    "    print(f\"Unique values and frequencies for column '{column}' saved to {output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:__main__:The following columns were not found in the dataset: {'Side', 'Number'}\n",
      "INFO:__main__:Successfully dropped 14 columns\n",
      "INFO:__main__:DataFrame shape changed from (7728394, 46) to (7728394, 32)\n",
      "INFO:fireducks.fallback:fallback_attr: name=Index.tolist reason=Index.__getattr__ for tolist is called \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Severity', 'Start_Time', 'End_Time', 'Start_Lat', 'Start_Lng', 'Distance(mi)', 'Description', 'City', 'County', 'State', 'Timezone', 'Temperature(F)', 'Humidity(%)', 'Pressure(in)', 'Visibility(mi)', 'Wind_Direction', 'Wind_Speed(mph)', 'Precipitation(in)', 'Weather_Condition', 'Amenity', 'Bump', 'Crossing', 'Give_Way', 'Junction', 'No_Exit', 'Railway', 'Roundabout', 'Station', 'Stop', 'Traffic_Calming', 'Traffic_Signal', 'Sunrise_Sunset']\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import logging\n",
    "\n",
    "def drop_redundant_columns(df):\n",
    "    \"\"\"\n",
    "    Drops redundant columns from the accidents dataset.\n",
    "    \n",
    "    Args:\n",
    "        df (pandas.DataFrame): Input DataFrame with accident data\n",
    "        \n",
    "    Returns:\n",
    "        pandas.DataFrame: DataFrame with redundant columns removed\n",
    "    \"\"\"\n",
    "    # Set up logging\n",
    "    logging.basicConfig(level=logging.INFO)\n",
    "    logger = logging.getLogger(__name__)\n",
    "    \n",
    "    # List of columns to drop\n",
    "    redundant_columns = [\n",
    "        'Source',\n",
    "        'End_Lat',\n",
    "        'End_Lng',\n",
    "        'Number',\n",
    "        'Street',\n",
    "        'Country',\n",
    "        'Airport_Code',\n",
    "        'Civil_Twilight',\n",
    "        'Nautical_Twilight',\n",
    "        'Astronomical_Twilight',\n",
    "        'Weather_Timestamp',\n",
    "        'ID',\n",
    "        'Turning_Loop',\n",
    "        'Wind_Chill(F)',\n",
    "        'Side',\n",
    "        'Zipcode'\n",
    "    ]\n",
    "    \n",
    "    # Store original shape\n",
    "    original_shape = df.shape\n",
    "    \n",
    "    try:\n",
    "        # Check which columns actually exist in the dataframe\n",
    "        columns_to_drop = [col for col in redundant_columns if col in df.columns]\n",
    "        \n",
    "        # Log any columns that were in our list but not in the dataframe\n",
    "        missing_columns = set(redundant_columns) - set(df.columns)\n",
    "        if missing_columns:\n",
    "            logger.warning(f\"The following columns were not found in the dataset: {missing_columns}\")\n",
    "        \n",
    "        # Drop the columns\n",
    "        df_cleaned = df.drop(columns=columns_to_drop)\n",
    "        \n",
    "        # Log the results\n",
    "        columns_dropped = len(columns_to_drop)\n",
    "        logger.info(f\"Successfully dropped {columns_dropped} columns\")\n",
    "        logger.info(f\"DataFrame shape changed from {original_shape} to {df_cleaned.shape}\")\n",
    "        \n",
    "        return df_cleaned\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"An error occurred while dropping columns: {str(e)}\")\n",
    "        raise\n",
    "    \n",
    "# Example usage:\n",
    "data = drop_redundant_columns(data)\n",
    "\n",
    "# To verify the remaining columns:\n",
    "print(data.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:fireducks.fallback:fallback_attr: name=Series.map reason=input 'arg' is neither a dict nor a type to be casted \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:fireducks.fallback:fallback_attr: name=Series.__repr__ reason=with full data of size: 11 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weather_Group\n",
      "Clear                  3405216\n",
      "Cloudy                 3163728\n",
      "Rain                    510564\n",
      "Poor Visibility         201398\n",
      "Unknown                 178798\n",
      "Snow                    160008\n",
      "Thunderstorm             88452\n",
      "Mixed Precipitation      13247\n",
      "Freezing Conditions       6049\n",
      "Dust/Sand                  747\n",
      "Severe Conditions          187\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def group_weather_conditions(weather_series):\n",
    "    \"\"\"\n",
    "    Groups weather conditions into broader categories.\n",
    "    \n",
    "    Args:\n",
    "        weather_series: pandas Series containing weather conditions\n",
    "        \n",
    "    Returns:\n",
    "        pandas Series with grouped weather conditions\n",
    "    \"\"\"\n",
    "    \n",
    "    # Create mapping dictionary for weather conditions\n",
    "    weather_mapping = {\n",
    "        # Clear conditions\n",
    "        'Fair': 'Clear',\n",
    "        'Clear': 'Clear',\n",
    "        'Fair / Windy': 'Clear',\n",
    "        \n",
    "        # Cloudy conditions\n",
    "        'Mostly Cloudy': 'Cloudy',\n",
    "        'Cloudy': 'Cloudy',\n",
    "        'Partly Cloudy': 'Cloudy',\n",
    "        'Overcast': 'Cloudy',\n",
    "        'Scattered Clouds': 'Cloudy',\n",
    "        'Cloudy / Windy': 'Cloudy',\n",
    "        'Mostly Cloudy / Windy': 'Cloudy',\n",
    "        'Partly Cloudy / Windy': 'Cloudy',\n",
    "        \n",
    "        # Rain conditions\n",
    "        'Light Rain': 'Rain',\n",
    "        'Rain': 'Rain',\n",
    "        'Heavy Rain': 'Rain',\n",
    "        'Light Drizzle': 'Rain',\n",
    "        'Light Rain / Windy': 'Rain',\n",
    "        'Rain / Windy': 'Rain',\n",
    "        'Heavy Rain / Windy': 'Rain',\n",
    "        'Drizzle': 'Rain',\n",
    "        'Light Rain Shower': 'Rain',\n",
    "        'Rain Shower': 'Rain',\n",
    "        'Rain Showers': 'Rain',\n",
    "        'Heavy Drizzle': 'Rain',\n",
    "        'Light Rain Showers': 'Rain',\n",
    "        'Heavy Rain Showers': 'Rain',\n",
    "        'Light Drizzle / Windy': 'Rain',\n",
    "        'Drizzle / Windy': 'Rain',\n",
    "        'Rain Shower / Windy': 'Rain',\n",
    "        'Heavy Rain Shower / Windy': 'Rain',\n",
    "        'Drizzle and Fog': 'Rain',\n",
    "        \n",
    "        # Snow conditions\n",
    "        'Light Snow': 'Snow',\n",
    "        'Snow': 'Snow',\n",
    "        'Heavy Snow': 'Snow',\n",
    "        'Light Snow / Windy': 'Snow',\n",
    "        'Snow / Windy': 'Snow',\n",
    "        'Heavy Snow / Windy': 'Snow',\n",
    "        'Blowing Snow': 'Snow',\n",
    "        'Blowing Snow / Windy': 'Snow',\n",
    "        'Light Snow Shower': 'Snow',\n",
    "        'Snow Showers': 'Snow',\n",
    "        'Light Snow Showers': 'Snow',\n",
    "        'Light Snow Shower / Windy': 'Snow',\n",
    "        'Snow Grains': 'Snow',\n",
    "        'Light Snow Grains': 'Snow',\n",
    "        'Drifting Snow': 'Snow',\n",
    "        'Drifting Snow / Windy': 'Snow',\n",
    "        'Low Drifting Snow': 'Snow',\n",
    "        'Heavy Blowing Snow': 'Snow',\n",
    "        'Light Blowing Snow': 'Snow',\n",
    "        'Blowing Snow Nearby': 'Snow',\n",
    "        \n",
    "        # Thunderstorm conditions\n",
    "        'Thunder': 'Thunderstorm',\n",
    "        'T-Storm': 'Thunderstorm',\n",
    "        'Heavy T-Storm': 'Thunderstorm',\n",
    "        'Thunder in the Vicinity': 'Thunderstorm',\n",
    "        'Light Rain with Thunder': 'Thunderstorm',\n",
    "        'Light Thunderstorms and Rain': 'Thunderstorm',\n",
    "        'Thunderstorm': 'Thunderstorm',\n",
    "        'Heavy Thunderstorms and Rain': 'Thunderstorm',\n",
    "        'Thunderstorms and Rain': 'Thunderstorm',\n",
    "        'Heavy T-Storm / Windy': 'Thunderstorm',\n",
    "        'T-Storm / Windy': 'Thunderstorm',\n",
    "        'Thunder / Windy': 'Thunderstorm',\n",
    "        'Thunder and Hail': 'Thunderstorm',\n",
    "        'Thunder and Hail / Windy': 'Thunderstorm',\n",
    "        'Light Thunderstorm': 'Thunderstorm',\n",
    "        \n",
    "        # Freezing conditions\n",
    "        'Light Freezing Rain': 'Freezing Conditions',\n",
    "        'Light Freezing Drizzle': 'Freezing Conditions',\n",
    "        'Freezing Rain': 'Freezing Conditions',\n",
    "        'Freezing Drizzle': 'Freezing Conditions',\n",
    "        'Heavy Freezing Rain': 'Freezing Conditions',\n",
    "        'Light Freezing Rain / Windy': 'Freezing Conditions',\n",
    "        'Freezing Rain / Windy': 'Freezing Conditions',\n",
    "        'Heavy Freezing Rain / Windy': 'Freezing Conditions',\n",
    "        'Heavy Freezing Drizzle': 'Freezing Conditions',\n",
    "        'Light Freezing Fog': 'Freezing Conditions',\n",
    "        \n",
    "        # Mixed precipitation\n",
    "        'Wintry Mix': 'Mixed Precipitation',\n",
    "        'Snow and Sleet': 'Mixed Precipitation',\n",
    "        'Light Snow and Sleet': 'Mixed Precipitation',\n",
    "        'Wintry Mix / Windy': 'Mixed Precipitation',\n",
    "        'Snow and Sleet / Windy': 'Mixed Precipitation',\n",
    "        'Light Snow and Sleet / Windy': 'Mixed Precipitation',\n",
    "        'Thunder / Wintry Mix': 'Mixed Precipitation',\n",
    "        'Thunder / Wintry Mix / Windy': 'Mixed Precipitation',\n",
    "        'Rain and Sleet': 'Mixed Precipitation',\n",
    "        'Sleet': 'Mixed Precipitation',\n",
    "        'Light Sleet': 'Mixed Precipitation',\n",
    "        'Heavy Sleet': 'Mixed Precipitation',\n",
    "        'Sleet / Windy': 'Mixed Precipitation',\n",
    "        'Light Sleet / Windy': 'Mixed Precipitation',\n",
    "        'Heavy Sleet / Windy': 'Mixed Precipitation',\n",
    "        \n",
    "        # Poor visibility conditions\n",
    "        'Fog': 'Poor Visibility',\n",
    "        'Haze': 'Poor Visibility',\n",
    "        'Smoke': 'Poor Visibility',\n",
    "        'Fog / Windy': 'Poor Visibility',\n",
    "        'Haze / Windy': 'Poor Visibility',\n",
    "        'Smoke / Windy': 'Poor Visibility',\n",
    "        'Patches of Fog': 'Poor Visibility',\n",
    "        'Mist': 'Poor Visibility',\n",
    "        'Shallow Fog': 'Poor Visibility',\n",
    "        'Patches of Fog / Windy': 'Poor Visibility',\n",
    "        'Mist / Windy': 'Poor Visibility',\n",
    "        'Shallow Fog / Windy': 'Poor Visibility',\n",
    "        'Light Haze': 'Poor Visibility',\n",
    "        'Light Fog': 'Poor Visibility',\n",
    "        'Partial Fog': 'Poor Visibility',\n",
    "        'Partial Fog / Windy': 'Poor Visibility',\n",
    "        'Heavy Smoke': 'Poor Visibility',\n",
    "        \n",
    "        # Severe conditions\n",
    "        'Tornado': 'Severe Conditions',\n",
    "        'Funnel Cloud': 'Severe Conditions',\n",
    "        'Small Hail': 'Severe Conditions',\n",
    "        'Hail': 'Severe Conditions',\n",
    "        'Light Hail': 'Severe Conditions',\n",
    "        'Volcanic Ash': 'Severe Conditions',\n",
    "        'Heavy Thunderstorms with Small Hail': 'Severe Conditions',\n",
    "        \n",
    "        # Dust/Sand conditions\n",
    "        'Blowing Dust': 'Dust/Sand',\n",
    "        'Widespread Dust': 'Dust/Sand',\n",
    "        'Sand / Dust Whirlwinds': 'Dust/Sand',\n",
    "        'Blowing Dust / Windy': 'Dust/Sand',\n",
    "        'Widespread Dust / Windy': 'Dust/Sand',\n",
    "        'Sand': 'Dust/Sand',\n",
    "        'Sand / Dust Whirls Nearby': 'Dust/Sand',\n",
    "        'Sand / Dust Whirlwinds / Windy': 'Dust/Sand',\n",
    "        'Duststorm': 'Dust/Sand',\n",
    "        'Blowing Sand': 'Dust/Sand',\n",
    "        'Dust Whirls': 'Dust/Sand',\n",
    "        'Sand / Windy': 'Dust/Sand'\n",
    "    }\n",
    "    \n",
    "    # Replace empty strings and N/A with 'Unknown'\n",
    "    weather_series = weather_series.fillna('Unknown')\n",
    "    weather_series = weather_series.replace('', 'Unknown')\n",
    "    weather_series = weather_series.replace('N/A Precipitation', 'Unknown')\n",
    "    \n",
    "    # Map the weather conditions to their groups\n",
    "    return weather_series.map(lambda x: weather_mapping.get(x, 'Unknown'))\n",
    "\n",
    "data['Weather_Group'] = group_weather_conditions(data['Weather_Condition'])\n",
    "data = data.drop(columns=['Weather_Condition'])\n",
    "# To see the distribution of the new groups\n",
    "print(data['Weather_Group'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:fireducks.fallback:fallback_attr: name=Series.map reason=input 'arg' is neither a dict nor a type to be casted \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:fireducks.fallback:fallback_attr: name=Series.__repr__ reason=with full data of size: 11 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wind_Direction_Group\n",
      "South        1331314\n",
      "Calm         1330181\n",
      "West         1281251\n",
      "North        1049274\n",
      "East          909244\n",
      "Northwest     369352\n",
      "Variable      364562\n",
      "Southwest     364470\n",
      "Southeast     294901\n",
      "Northeast     258639\n",
      "Unknown       175206\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def group_wind_directions(wind_series):\n",
    "    \"\"\"\n",
    "    Groups wind directions into 8 main cardinal directions.\n",
    "    \n",
    "    Args:\n",
    "        wind_series: pandas Series containing wind directions\n",
    "        \n",
    "    Returns:\n",
    "        pandas Series with grouped wind directions\n",
    "    \"\"\"\n",
    "    \n",
    "    # Create mapping dictionary for wind directions\n",
    "    wind_mapping = {\n",
    "        # North (337.5° - 22.5°)\n",
    "        'N': 'North',\n",
    "        'North': 'North',\n",
    "        'NNE': 'North',\n",
    "        'NNW': 'North',\n",
    "        \n",
    "        # East (67.5° - 112.5°)\n",
    "        'E': 'East',\n",
    "        'East': 'East',\n",
    "        'ENE': 'East',\n",
    "        'ESE': 'East',\n",
    "        \n",
    "        # South (157.5° - 202.5°)\n",
    "        'S': 'South',\n",
    "        'South': 'South',\n",
    "        'SSE': 'South',\n",
    "        'SSW': 'South',\n",
    "        \n",
    "        # West (247.5° - 292.5°)\n",
    "        'W': 'West',\n",
    "        'West': 'West',\n",
    "        'WNW': 'West',\n",
    "        'WSW': 'West',\n",
    "        \n",
    "        # Northeast (22.5° - 67.5°)\n",
    "        'NE': 'Northeast',\n",
    "        \n",
    "        # Southeast (112.5° - 157.5°)\n",
    "        'SE': 'Southeast',\n",
    "        \n",
    "        # Southwest (202.5° - 247.5°)\n",
    "        'SW': 'Southwest',\n",
    "        \n",
    "        # Northwest (292.5° - 337.5°)\n",
    "        'NW': 'Northwest',\n",
    "        \n",
    "        # Calm/Variable conditions\n",
    "        'CALM': 'Calm',\n",
    "        'Calm': 'Calm',\n",
    "        'VAR': 'Variable',\n",
    "        'Variable': 'Variable'\n",
    "    }\n",
    "    \n",
    "    # Replace empty strings and missing values with 'Unknown'\n",
    "    wind_series = wind_series.fillna('Unknown')\n",
    "    wind_series = wind_series.replace('', 'Unknown')\n",
    "    \n",
    "    # Map the wind directions to their groups\n",
    "    return wind_series.map(lambda x: wind_mapping.get(x, 'Unknown'))\n",
    "\n",
    "\n",
    "data['Wind_Direction_Group'] = group_wind_directions(data['Wind_Direction'])\n",
    "data.drop(columns=['Wind_Direction'], inplace=True)\n",
    "# To see the distribution of the new groups\n",
    "print(data['Wind_Direction_Group'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:fireducks.fallback:fallback_attr: name=Series._typ reason=Series.__getattr__ for _typ is called \n",
      "INFO:fireducks.fallback:fallback_attr: name=Series.__iter__ reason=Installed fallback wrapper for Series.__iter__ is called \n",
      "INFO:fireducks.fallback:fallback_attr: name=Series._values reason=Series.__getattr__ for _values is called \n",
      "INFO:fireducks.fallback:fallback_attr: name=Series._constructor reason=Series.__getattr__ for _constructor is called \n",
      "INFO:fireducks.fallback:fallback_attr: name=DataFrame.__setitem__ reason=value is not FireDucksPandasCompat \n",
      "INFO:fireducks.fallback:fallback_attr: name=RangeIndex._can_hold_identifiers_and_holds_name reason=RangeIndex.__getattr__ for _can_hold_identifiers_and_holds_name is called \n",
      "INFO:fireducks.fallback:fallback_attr: name=RangeIndex.equals reason=RangeIndex.__getattr__ for equals is called \n",
      "INFO:fireducks.fallback:fallback_attr: name=Series._typ reason=Series.__getattr__ for _typ is called \n",
      "INFO:fireducks.fallback:fallback_attr: name=Series.__iter__ reason=Installed fallback wrapper for Series.__iter__ is called \n",
      "INFO:fireducks.fallback:fallback_attr: name=Series._values reason=Series.__getattr__ for _values is called \n",
      "INFO:fireducks.fallback:fallback_attr: name=Series._constructor reason=Series.__getattr__ for _constructor is called \n",
      "INFO:fireducks.fallback:fallback_attr: name=DataFrame.__setitem__ reason=value is not FireDucksPandasCompat \n",
      "INFO:fireducks.fallback:fallback_attr: name=RangeIndex._can_hold_identifiers_and_holds_name reason=RangeIndex.__getattr__ for _can_hold_identifiers_and_holds_name is called \n",
      "INFO:fireducks.fallback:fallback_attr: name=RangeIndex.equals reason=RangeIndex.__getattr__ for equals is called \n",
      "INFO:fireducks.fallback:fallback_attr: name=Series.apply reason=None \n",
      "INFO:fireducks.fallback:fallback_attr: name=Series.__repr__ reason=with full data of size: 5 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time of day distribution:\n",
      "Time_Of_Day\n",
      "Morning      2859847\n",
      "Afternoon    1683839\n",
      "Noon         1200292\n",
      "Evening      1143841\n",
      "Night         840575\n",
      "Name: count, dtype: int64\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:fireducks.fallback:fallback_attr: name=Series.__repr__ reason=with full data of size: 8 \n",
      "INFO:fireducks.fallback:fallback_attr: name=DataFrame.__repr__ reason=with full data of size: 5 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Duration statistics (in minutes):\n",
      "count    7.728394e+06\n",
      "mean     4.444210e+02\n",
      "std      1.351229e+04\n",
      "min      1.220000e+00\n",
      "25%      3.150000e+01\n",
      "50%      7.483000e+01\n",
      "75%      1.251500e+02\n",
      "max      2.812939e+06\n",
      "Name: Duration_Minutes, dtype: float64\n",
      "\n",
      "Sample of processed data:\n",
      "           Start_Time            End_Time  Year  Month  Day Time_Of_Day  \\\n",
      "0 2016-02-08 05:46:00 2016-02-08 11:00:00  2016      2    8     Morning   \n",
      "1 2016-02-08 06:07:59 2016-02-08 06:37:59  2016      2    8     Morning   \n",
      "2 2016-02-08 06:49:27 2016-02-08 07:19:27  2016      2    8     Morning   \n",
      "3 2016-02-08 07:23:34 2016-02-08 07:53:34  2016      2    8     Morning   \n",
      "4 2016-02-08 07:39:07 2016-02-08 08:09:07  2016      2    8     Morning   \n",
      "\n",
      "   Duration_Minutes  \n",
      "0             314.0  \n",
      "1              30.0  \n",
      "2              30.0  \n",
      "3              30.0  \n",
      "4              30.0  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "\n",
    "def categorize_time_of_day(hour):\n",
    "    \"\"\"\n",
    "    Categorizes the hour into time of day\n",
    "    \n",
    "    Args:\n",
    "        hour: int (0-23)\n",
    "    Returns:\n",
    "        str: time of day category\n",
    "    \"\"\"\n",
    "    if 5 <= hour < 12:\n",
    "        return 'Morning'\n",
    "    elif 12 <= hour < 15:\n",
    "        return 'Noon'\n",
    "    elif 15 <= hour < 18:\n",
    "        return 'Afternoon'\n",
    "    elif 18 <= hour < 22:\n",
    "        return 'Evening'\n",
    "    else:\n",
    "        return 'Night'\n",
    "\n",
    "def process_time_features(df):\n",
    "    \"\"\"\n",
    "    Process time-related features:\n",
    "    1. Extract time of day from Start_Time\n",
    "    2. Calculate accident duration in minutes\n",
    "    3. Drop Sunrise_Sunset column\n",
    "    4. Extract year, month, day from Start_Time\n",
    "    \n",
    "    Args:\n",
    "        df: pandas DataFrame with Start_Time and End_Time columns\n",
    "    Returns:\n",
    "        pandas DataFrame with new time features\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Convert Start_Time and End_Time to datetime using a more flexible parser\n",
    "        df['Start_Time'] = pd.to_datetime(df['Start_Time'], format='mixed')\n",
    "        df['End_Time'] = pd.to_datetime(df['End_Time'], format='mixed')\n",
    "        \n",
    "        # Extract year, month, and day from Start_Time\n",
    "        df['Year'] = df['Start_Time'].dt.year\n",
    "        df['Month'] = df['Start_Time'].dt.month\n",
    "        df['Day'] = df['Start_Time'].dt.day\n",
    "        \n",
    "        # Extract hour from Start_Time\n",
    "        df['Hour'] = df['Start_Time'].dt.hour\n",
    "        \n",
    "        # Add Time_Of_Day column\n",
    "        df['Time_Of_Day'] = df['Hour'].apply(categorize_time_of_day)\n",
    "        \n",
    "        # Calculate duration in minutes\n",
    "        df['Duration_Minutes'] = ((df['End_Time'] - df['Start_Time']).dt.total_seconds() / 60).round(2)\n",
    "        \n",
    "        # Drop intermediate and redundant columns\n",
    "        columns_to_drop = ['Hour', 'Sunrise_Sunset']\n",
    "        existing_columns = [col for col in columns_to_drop if col in df.columns]\n",
    "        df = df.drop(columns=existing_columns)\n",
    "        \n",
    "        # Print summary statistics\n",
    "        print(f\"Time of day distribution:\\n{df['Time_Of_Day'].value_counts()}\\n\")\n",
    "        print(f\"Duration statistics (in minutes):\\n{df['Duration_Minutes'].describe()}\")\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing time features: {str(e)}\")\n",
    "        # Print some example problematic rows if there's an error\n",
    "        if 'Start_Time' in df.columns:\n",
    "            print(\"\\nExample timestamp formats in the data:\")\n",
    "            print(df['Start_Time'].head(5))\n",
    "        raise\n",
    "\n",
    "# Process the time features\n",
    "data = process_time_features(data)\n",
    "\n",
    "# To verify the results\n",
    "print(\"\\nSample of processed data:\")\n",
    "print(data[['Start_Time', 'End_Time', 'Year', 'Month', 'Day', 'Time_Of_Day', 'Duration_Minutes']].head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.drop(columns=['Start_Time', 'End_Time'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:fireducks.fallback:fallback_attr: name=DataFrame.select_dtypes reason=DataFrame.__getattr__ for select_dtypes is called \n",
      "INFO:fireducks.fallback:fallback_attr: name=Series.__repr__ reason=with full data of size: 8 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Description             3761579\n",
      "City                      13679\n",
      "County                     1871\n",
      "State                        49\n",
      "Timezone                      5\n",
      "Weather_Group                11\n",
      "Wind_Direction_Group         11\n",
      "Time_Of_Day                   5\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Get the number of unique values for each object (non-numeric) column\n",
    "unique_counts = data.select_dtypes(include=['object']).nunique()\n",
    "\n",
    "# Print the unique counts for each object column\n",
    "print(unique_counts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:fireducks.fallback:fallback_attr: name=Series.__repr__ reason=with full data of size: 10 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    Right lane blocked due to accident on I-70 Eas...\n",
      "1    Accident on Brice Rd at Tussing Rd. Expect del...\n",
      "2    Accident on OH-32 State Route 32 Westbound at ...\n",
      "3    Accident on I-75 Southbound at Exits 52 52B US...\n",
      "4    Accident on McEwen Rd at OH-725 Miamisburg Cen...\n",
      "5    Accident on I-270 Outerbelt Northbound near Ex...\n",
      "6    Accident on Oakridge Dr at Woodward Ave. Expec...\n",
      "7    Accident on I-75 Southbound at Exit 54B Grand ...\n",
      "8    Accident on Notre Dame Ave at Warner Ave. Expe...\n",
      "9    Right hand shoulder blocked due to accident on...\n",
      "Name: Description, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(data['Description'].head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:fireducks.fallback:fallback_attr: name=Series.apply reason=None \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(12.248763067721443, 119, 1)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['Description_word_length'] = data['Description'].apply(lambda x: len(str(x).split()))\n",
    "avg_word_length = data['Description_word_length'].mean()\n",
    "max_word_length = data['Description_word_length'].max()\n",
    "min_word_length = data['Description_word_length'].min()\n",
    "\n",
    "avg_word_length, max_word_length, min_word_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:fireducks.fallback:fallback_attr: name=Series.__repr__ reason=with full data of size: 34 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of null values in each column:\n",
      "\n",
      "Severity                      0\n",
      "Start_Lat                     0\n",
      "Start_Lng                     0\n",
      "Distance(mi)                  0\n",
      "Description                   5\n",
      "City                        253\n",
      "County                        0\n",
      "State                         0\n",
      "Timezone                   7808\n",
      "Temperature(F)           163853\n",
      "Humidity(%)              174144\n",
      "Pressure(in)             140679\n",
      "Visibility(mi)           177098\n",
      "Wind_Speed(mph)          571233\n",
      "Precipitation(in)       2203586\n",
      "Amenity                       0\n",
      "Bump                          0\n",
      "Crossing                      0\n",
      "Give_Way                      0\n",
      "Junction                      0\n",
      "No_Exit                       0\n",
      "Railway                       0\n",
      "Roundabout                    0\n",
      "Station                       0\n",
      "Stop                          0\n",
      "Traffic_Calming               0\n",
      "Traffic_Signal                0\n",
      "Weather_Group                 0\n",
      "Wind_Direction_Group          0\n",
      "Year                          0\n",
      "Month                         0\n",
      "Day                           0\n",
      "Time_Of_Day                   0\n",
      "Duration_Minutes              0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Check the number of null values in each column\n",
    "null_values = data.isnull().sum()\n",
    "print(\"Number of null values in each column:\\n\")\n",
    "print(null_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.to_csv('/teamspace/studios/this_studio/Assignment-TechstaX/data/cleaned_accidents_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Severity</th>\n",
       "      <th>Start_Lat</th>\n",
       "      <th>Start_Lng</th>\n",
       "      <th>Distance(mi)</th>\n",
       "      <th>Description</th>\n",
       "      <th>City</th>\n",
       "      <th>County</th>\n",
       "      <th>State</th>\n",
       "      <th>Timezone</th>\n",
       "      <th>Temperature(F)</th>\n",
       "      <th>...</th>\n",
       "      <th>Stop</th>\n",
       "      <th>Traffic_Calming</th>\n",
       "      <th>Traffic_Signal</th>\n",
       "      <th>Weather_Group</th>\n",
       "      <th>Wind_Direction_Group</th>\n",
       "      <th>Year</th>\n",
       "      <th>Month</th>\n",
       "      <th>Day</th>\n",
       "      <th>Time_Of_Day</th>\n",
       "      <th>Duration_Minutes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>839272</th>\n",
       "      <td>2</td>\n",
       "      <td>34.164249</td>\n",
       "      <td>-117.333122</td>\n",
       "      <td>0.000</td>\n",
       "      <td>Right hand shoulder blocked due to accident on...</td>\n",
       "      <td>San Bernardino</td>\n",
       "      <td>San Bernardino</td>\n",
       "      <td>CA</td>\n",
       "      <td>US/Pacific</td>\n",
       "      <td>59.0</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>Cloudy</td>\n",
       "      <td>Calm</td>\n",
       "      <td>2021</td>\n",
       "      <td>10</td>\n",
       "      <td>23</td>\n",
       "      <td>Morning</td>\n",
       "      <td>74.68</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5892959</th>\n",
       "      <td>2</td>\n",
       "      <td>30.335065</td>\n",
       "      <td>-97.587959</td>\n",
       "      <td>0.695</td>\n",
       "      <td>Incident on N TX-130 NB near E US-290 Drive wi...</td>\n",
       "      <td>Manor</td>\n",
       "      <td>Travis</td>\n",
       "      <td>TX</td>\n",
       "      <td>US/Central</td>\n",
       "      <td>57.0</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>Clear</td>\n",
       "      <td>North</td>\n",
       "      <td>2021</td>\n",
       "      <td>11</td>\n",
       "      <td>25</td>\n",
       "      <td>Noon</td>\n",
       "      <td>78.62</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5264453</th>\n",
       "      <td>2</td>\n",
       "      <td>28.022854</td>\n",
       "      <td>-82.455735</td>\n",
       "      <td>0.986</td>\n",
       "      <td>Incident on I-275 near MM 44 Drive with caution.</td>\n",
       "      <td>Tampa</td>\n",
       "      <td>Hillsborough</td>\n",
       "      <td>FL</td>\n",
       "      <td>US/Eastern</td>\n",
       "      <td>81.0</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>Cloudy</td>\n",
       "      <td>East</td>\n",
       "      <td>2022</td>\n",
       "      <td>12</td>\n",
       "      <td>2</td>\n",
       "      <td>Noon</td>\n",
       "      <td>175.42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1305959</th>\n",
       "      <td>3</td>\n",
       "      <td>45.290531</td>\n",
       "      <td>-93.768631</td>\n",
       "      <td>0.000</td>\n",
       "      <td>Right lane blocked due to accident on I-94 Eas...</td>\n",
       "      <td>Monticello</td>\n",
       "      <td>Wright</td>\n",
       "      <td>MN</td>\n",
       "      <td>US/Central</td>\n",
       "      <td>61.0</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>Clear</td>\n",
       "      <td>West</td>\n",
       "      <td>2020</td>\n",
       "      <td>9</td>\n",
       "      <td>27</td>\n",
       "      <td>Noon</td>\n",
       "      <td>60.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1844459</th>\n",
       "      <td>2</td>\n",
       "      <td>43.322029</td>\n",
       "      <td>-88.247635</td>\n",
       "      <td>0.000</td>\n",
       "      <td>Right hand shoulder blocked due to accident on...</td>\n",
       "      <td>Slinger</td>\n",
       "      <td>Washington</td>\n",
       "      <td>WI</td>\n",
       "      <td>US/Central</td>\n",
       "      <td>61.0</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>Poor Visibility</td>\n",
       "      <td>Southeast</td>\n",
       "      <td>2019</td>\n",
       "      <td>9</td>\n",
       "      <td>30</td>\n",
       "      <td>Morning</td>\n",
       "      <td>130.97</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1169930</th>\n",
       "      <td>2</td>\n",
       "      <td>33.740601</td>\n",
       "      <td>-84.412041</td>\n",
       "      <td>0.000</td>\n",
       "      <td>Restrictions due to accident on US-29 Whitehal...</td>\n",
       "      <td>Atlanta</td>\n",
       "      <td>Fulton</td>\n",
       "      <td>GA</td>\n",
       "      <td>US/Eastern</td>\n",
       "      <td>66.0</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>Thunderstorm</td>\n",
       "      <td>West</td>\n",
       "      <td>2021</td>\n",
       "      <td>1</td>\n",
       "      <td>26</td>\n",
       "      <td>Morning</td>\n",
       "      <td>29.60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>524706</th>\n",
       "      <td>2</td>\n",
       "      <td>33.960789</td>\n",
       "      <td>-117.312996</td>\n",
       "      <td>0.000</td>\n",
       "      <td>#1 lane blocked and HOV lane blocked due to cr...</td>\n",
       "      <td>Riverside</td>\n",
       "      <td>Riverside</td>\n",
       "      <td>CA</td>\n",
       "      <td>US/Pacific</td>\n",
       "      <td>90.0</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>Clear</td>\n",
       "      <td>Variable</td>\n",
       "      <td>2022</td>\n",
       "      <td>8</td>\n",
       "      <td>26</td>\n",
       "      <td>Noon</td>\n",
       "      <td>44.87</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1147334</th>\n",
       "      <td>2</td>\n",
       "      <td>37.449219</td>\n",
       "      <td>-77.627533</td>\n",
       "      <td>0.000</td>\n",
       "      <td>Accident on VA-604 Genito Rd at Warbro Rd.</td>\n",
       "      <td>Midlothian</td>\n",
       "      <td>Chesterfield</td>\n",
       "      <td>VA</td>\n",
       "      <td>US/Eastern</td>\n",
       "      <td>36.0</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>Mixed Precipitation</td>\n",
       "      <td>Northeast</td>\n",
       "      <td>2021</td>\n",
       "      <td>2</td>\n",
       "      <td>11</td>\n",
       "      <td>Morning</td>\n",
       "      <td>61.35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>524845</th>\n",
       "      <td>3</td>\n",
       "      <td>32.762260</td>\n",
       "      <td>-96.958298</td>\n",
       "      <td>0.000</td>\n",
       "      <td>Four lanes blocked due to crash on I-30 Eastbo...</td>\n",
       "      <td>Grand Prairie</td>\n",
       "      <td>Dallas</td>\n",
       "      <td>TX</td>\n",
       "      <td>US/Central</td>\n",
       "      <td>76.0</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>Clear</td>\n",
       "      <td>Calm</td>\n",
       "      <td>2022</td>\n",
       "      <td>8</td>\n",
       "      <td>26</td>\n",
       "      <td>Night</td>\n",
       "      <td>28.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3680558</th>\n",
       "      <td>2</td>\n",
       "      <td>33.722011</td>\n",
       "      <td>-117.798861</td>\n",
       "      <td>0.856</td>\n",
       "      <td>Incident on I-5 SB near CHAPMAN AVE Drive with...</td>\n",
       "      <td>Tustin</td>\n",
       "      <td>Orange</td>\n",
       "      <td>CA</td>\n",
       "      <td>US/Pacific</td>\n",
       "      <td>66.0</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>Cloudy</td>\n",
       "      <td>Calm</td>\n",
       "      <td>2022</td>\n",
       "      <td>11</td>\n",
       "      <td>17</td>\n",
       "      <td>Morning</td>\n",
       "      <td>78.60</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 34 columns</p>\n",
       "</div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data = pd.read_csv(\"/teamspace/studios/this_studio/Assignment-TechstaX/data/cleaned_accidents_data.csv\")\n",
    "data.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Number of rows with no null values: 5400370\n"
     ]
    }
   ],
   "source": [
    "# Count the number of rows with no null values\n",
    "rows_with_no_nulls = data.dropna().shape[0]\n",
    "print(\"\\nNumber of rows with no null values:\", rows_with_no_nulls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7728394, 34)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Rows: 7728394\n",
      "Training Data Shape: (5796296, 34)\n",
      "Validation Data Shape: (1159259, 34)\n",
      "Test Data Shape: (772839, 34)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Assuming 'data' is your DataFrame and you want to split based on non-null values\n",
    "\n",
    "# Total rows in the dataset\n",
    "total_rows = data.shape[0]\n",
    "\n",
    "# Step 1: Separate rows with no null values for validation and test sets\n",
    "non_null_data = data.dropna()  # This will give rows with no null values\n",
    "total_non_null_rows = non_null_data.shape[0]\n",
    "\n",
    "# Calculate the required number of rows for validation (15%) and test (10%) based on total rows\n",
    "val_size = int(0.15 * total_rows)  # 15% of total data\n",
    "test_size = int(0.10 * total_rows)  # 10% of total data\n",
    "\n",
    "# Step 2: Split the non-null data into validation and test sets based on the required number of rows\n",
    "val_non_null = non_null_data.sample(n=val_size, random_state=42)\n",
    "test_non_null = non_null_data.drop(val_non_null.index).sample(n=test_size, random_state=42)\n",
    "\n",
    "# Step 3: Separate rows with null values for training\n",
    "train_with_nulls = data[~data.index.isin(non_null_data.index)]  # Rows with null values\n",
    "\n",
    "# Step 4: Combine the non-null rows for training (excluding those in validation or test) with the rows containing null values\n",
    "train_non_null = non_null_data.drop(val_non_null.index).drop(test_non_null.index)\n",
    "train_data = pd.concat([train_non_null, train_with_nulls])\n",
    "\n",
    "# Step 5: Verify the splits\n",
    "print(f\"Total Rows: {total_rows}\")\n",
    "print(f\"Training Data Shape: {train_data.shape}\")\n",
    "print(f\"Validation Data Shape: {val_non_null.shape}\")\n",
    "print(f\"Test Data Shape: {test_non_null.shape}\")\n",
    "\n",
    "# You can now proceed with your preprocessing on `train_data`, and validation/test data are ready.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files saved successfully.\n"
     ]
    }
   ],
   "source": [
    "# Save the datasets to CSV\n",
    "train_data.to_csv('/teamspace/studios/this_studio/Assignment-TechstaX/data/train_data.csv', index=False)\n",
    "val_non_null.to_csv('/teamspace/studios/this_studio/Assignment-TechstaX/data/val_data.csv', index=False)\n",
    "test_non_null.to_csv('/teamspace/studios/this_studio/Assignment-TechstaX/data/test_data.csv', index=False)\n",
    "\n",
    "# Confirm that the files have been saved\n",
    "print(\"Files saved successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
