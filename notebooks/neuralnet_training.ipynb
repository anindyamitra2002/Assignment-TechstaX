{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install pytorch-lightning transformers wandb scikit-learn pandas numpy torch h5py --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from sklearn.utils import shuffle\n",
    "from typing import List, Tuple\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import pytorch_lightning as pl\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint, EarlyStopping, TQDMProgressBar\n",
    "from pytorch_lightning.loggers import WandbLogger\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import wandb\n",
    "from sklearn.metrics import classification_report, accuracy_score, f1_score\n",
    "from typing import Dict, Tuple\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "class LitProgressBar(TQDMProgressBar):\n",
    "    def init_validation_tqdm(self):\n",
    "        bar = super().init_validation_tqdm()\n",
    "        bar.set_description('Validation')\n",
    "        return bar\n",
    "    \n",
    "    def init_train_tqdm(self):\n",
    "        bar = super().init_train_tqdm()\n",
    "        bar.set_description('Training')\n",
    "        return bar\n",
    "    \n",
    "    \n",
    "class AccidentPredictor(pl.LightningModule):\n",
    "    def __init__(self, \n",
    "                 tabular_dim: int,\n",
    "                 embedding_dim: int,\n",
    "                 num_classes: int,\n",
    "                 learning_rate: float = 1e-3):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "        \n",
    "        # Network architecture\n",
    "        self.tabular_network = nn.Sequential(\n",
    "            nn.Linear(tabular_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2)\n",
    "        )\n",
    "        \n",
    "        self.text_network = nn.Sequential(\n",
    "            nn.Linear(embedding_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2)\n",
    "        )\n",
    "        \n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(64, num_classes)\n",
    "        )\n",
    "        \n",
    "        self.criterion = nn.CrossEntropyLoss()\n",
    "        \n",
    "    def forward(self, tabular_features, text_embedding):\n",
    "        tabular_features = self.tabular_network(tabular_features)\n",
    "        text_features = self.text_network(text_embedding)\n",
    "        combined = torch.cat([tabular_features, text_features], dim=1)\n",
    "        return self.classifier(combined)\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        tabular_features, text_embedding, labels = batch\n",
    "        logits = self(tabular_features, text_embedding)\n",
    "        loss = self.criterion(logits, labels)\n",
    "        \n",
    "        preds = torch.argmax(logits, dim=1)\n",
    "        acc = accuracy_score(labels.cpu(), preds.cpu())\n",
    "        f1 = f1_score(labels.cpu(), preds.cpu(), average='weighted')\n",
    "        \n",
    "        self.log('train_loss', loss, prog_bar=True)\n",
    "        self.log('train_acc', acc, prog_bar=True)\n",
    "        self.log('train_f1', f1, prog_bar=True)\n",
    "        \n",
    "        return loss\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        tabular_features, text_embedding, labels = batch\n",
    "        logits = self(tabular_features, text_embedding)\n",
    "        loss = self.criterion(logits, labels)\n",
    "        \n",
    "        preds = torch.argmax(logits, dim=1)\n",
    "        acc = accuracy_score(labels.cpu(), preds.cpu())\n",
    "        f1 = f1_score(labels.cpu(), preds.cpu(), average='weighted')\n",
    "        \n",
    "        self.log('val_loss', loss, prog_bar=True)\n",
    "        self.log('val_acc', acc, prog_bar=True)\n",
    "        self.log('val_f1', f1, prog_bar=True)\n",
    "        \n",
    "        return loss\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.parameters(), \n",
    "                                   lr=self.hparams.learning_rate)\n",
    "        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "            optimizer, mode='min', factor=0.1, patience=5, verbose=True\n",
    "        )\n",
    "        return {\n",
    "            'optimizer': optimizer,\n",
    "            'lr_scheduler': scheduler,\n",
    "            'monitor': 'val_loss'\n",
    "        }\n",
    "        \n",
    "def split_and_save_data(train_df: pd.DataFrame, \n",
    "                       val_df: pd.DataFrame,\n",
    "                       train_chunk_size: int = 500000,\n",
    "                       val_chunk_size: int = 100000,\n",
    "                       output_dir: str = 'data_chunks') -> Tuple[List[str], List[str]]:\n",
    "    \"\"\"\n",
    "    Split and save training and validation data into smaller chunks\n",
    "    \"\"\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    # Shuffle the dataframes\n",
    "    train_df = shuffle(train_df, random_state=42)\n",
    "    val_df = shuffle(val_df, random_state=42)\n",
    "    \n",
    "    # Split and save training data\n",
    "    train_chunks = []\n",
    "    for i in range(0, len(train_df), train_chunk_size):\n",
    "        chunk = train_df.iloc[i:i + train_chunk_size]\n",
    "        filename = f'{output_dir}/train_chunk_{i//train_chunk_size}.csv'\n",
    "        chunk.to_csv(filename, index=False)\n",
    "        train_chunks.append(filename)\n",
    "    \n",
    "    # Split and save validation data\n",
    "    val_chunks = []\n",
    "    for i in range(0, len(val_df), val_chunk_size):\n",
    "        chunk = val_df.iloc[i:i + val_chunk_size]\n",
    "        filename = f'{output_dir}/val_chunk_{i//val_chunk_size}.csv'\n",
    "        chunk.to_csv(filename, index=False)\n",
    "        val_chunks.append(filename)\n",
    "    \n",
    "    return train_chunks, val_chunks\n",
    "\n",
    "def train_model_incrementally(train_chunks: List[str],\n",
    "                            val_chunks: List[str],\n",
    "                            model_dir: str = 'models',\n",
    "                            batch_size: int = 32,\n",
    "                            epochs_per_chunk: int = 10):\n",
    "    \"\"\"\n",
    "    Train the model incrementally on data chunks\n",
    "    \"\"\"\n",
    "    os.makedirs(model_dir, exist_ok=True)\n",
    "    tokenizer = AutoTokenizer.from_pretrained('BAAI/bge-small-en-v1.5')\n",
    "    \n",
    "    model = None\n",
    "    label_encoder = None\n",
    "    \n",
    "    for chunk_idx, (train_chunk, val_chunk) in enumerate(zip(train_chunks, val_chunks)):\n",
    "        print(f\"\\nTraining on chunk {chunk_idx + 1}/{len(train_chunks)}\")\n",
    "        \n",
    "        # Load data chunks\n",
    "        train_df = pd.read_csv(train_chunk)\n",
    "        val_df = pd.read_csv(val_chunk)\n",
    "        \n",
    "        # Create datasets\n",
    "        if model is None:  # First chunk\n",
    "            train_dataset = AccidentDataset(train_df, tokenizer)\n",
    "            val_dataset = AccidentDataset(val_df, tokenizer,\n",
    "                                        text_embeddings=train_dataset.get_text_embeddings(val_df['Description'].tolist()))\n",
    "            \n",
    "            # Initialize model\n",
    "            model = AccidentPredictor(\n",
    "                tabular_dim=train_dataset.tabular_features.shape[1],\n",
    "                embedding_dim=train_dataset.text_embeddings.shape[1],\n",
    "                num_classes=len(train_dataset.label_encoder.classes_)\n",
    "            )\n",
    "            label_encoder = train_dataset.label_encoder\n",
    "        else:  # Subsequent chunks\n",
    "            train_dataset = AccidentDataset(train_df, tokenizer, label_encoder=label_encoder)\n",
    "            val_dataset = AccidentDataset(val_df, tokenizer,\n",
    "                                        text_embeddings=train_dataset.get_text_embeddings(val_df['Description'].tolist()),\n",
    "                                        label_encoder=label_encoder)\n",
    "        \n",
    "        # Create dataloaders\n",
    "        train_loader = DataLoader(train_dataset, \n",
    "                                batch_size=batch_size, \n",
    "                                shuffle=True, \n",
    "                                num_workers=4,\n",
    "                                pin_memory=True)\n",
    "        val_loader = DataLoader(val_dataset, \n",
    "                              batch_size=batch_size, \n",
    "                              shuffle=False, \n",
    "                              num_workers=4,\n",
    "                              pin_memory=True)\n",
    "        \n",
    "        # Initialize wandb logger with unique name for each chunk\n",
    "        wandb_logger = WandbLogger(project='accident-severity-prediction',\n",
    "                                 name=f'hybrid-model-chunk-{chunk_idx}')\n",
    "        \n",
    "        # Initialize trainer\n",
    "        trainer = pl.Trainer(\n",
    "            max_epochs=epochs_per_chunk,\n",
    "            logger=wandb_logger,\n",
    "            callbacks=[\n",
    "                ModelCheckpoint(\n",
    "                    dirpath=f'{model_dir}/chunk_{chunk_idx}',\n",
    "                    filename='accident-predictor-{epoch:02d}-{val_loss:.2f}',\n",
    "                    save_top_k=1,\n",
    "                    mode='min'\n",
    "                ),\n",
    "                EarlyStopping(monitor='val_loss', patience=5, mode='min'),\n",
    "                LitProgressBar()\n",
    "            ],\n",
    "            accelerator='auto',\n",
    "            devices=1,\n",
    "            log_every_n_steps=10\n",
    "        )\n",
    "        \n",
    "        # Train model\n",
    "        trainer.fit(model, train_loader, val_loader)\n",
    "        \n",
    "        # Save model after each chunk\n",
    "        torch.save({\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'label_encoder': label_encoder\n",
    "        }, f'{model_dir}/model_after_chunk_{chunk_idx}.pt')\n",
    "        \n",
    "        # Clear memory\n",
    "        del train_dataset, val_dataset, train_loader, val_loader\n",
    "        torch.cuda.empty_cache()\n",
    "    \n",
    "    return model, label_encoder\n",
    "\n",
    "# Modified AccidentDataset class\n",
    "class AccidentDataset(Dataset):\n",
    "    def __init__(self, df: pd.DataFrame, tokenizer, text_embeddings=None, \n",
    "                 is_test: bool = False, label_encoder=None):\n",
    "        self.df = df\n",
    "        self.tokenizer = tokenizer\n",
    "        self.is_test = is_test\n",
    "        \n",
    "        # Process tabular features\n",
    "        print(\"Processing tabular features...\")\n",
    "        self.process_tabular_features()\n",
    "        \n",
    "        # Store or compute text embeddings\n",
    "        if text_embeddings is not None:\n",
    "            self.text_embeddings = text_embeddings\n",
    "        else:\n",
    "            print(\"Computing text embeddings...\")\n",
    "            self.text_embeddings = self.get_text_embeddings(df['Description'].tolist())\n",
    "        \n",
    "        if not is_test:\n",
    "            if label_encoder is None:\n",
    "                self.label_encoder = LabelEncoder()\n",
    "                self.labels = self.label_encoder.fit_transform(df['Severity'])\n",
    "            else:\n",
    "                self.label_encoder = label_encoder\n",
    "                self.labels = self.label_encoder.transform(df['Severity'])\n",
    "    \n",
    "    def process_tabular_features(self):\n",
    "        # Select all columns except 'Severity' and 'Description'\n",
    "        tabular_cols = [col for col in self.df.columns \n",
    "                       if col not in ['Severity', 'Description']]\n",
    "        \n",
    "        # Convert boolean columns to int\n",
    "        bool_cols = self.df[tabular_cols].select_dtypes(include=['bool']).columns\n",
    "        for col in bool_cols:\n",
    "            self.df[col] = self.df[col].astype(int)\n",
    "        \n",
    "        # Convert categorical columns to numeric using label encoding\n",
    "        cat_cols = self.df[tabular_cols].select_dtypes(include=['object']).columns\n",
    "        self.label_encoders = {}\n",
    "        for col in tqdm(cat_cols, desc=\"Encoding categorical columns\"):\n",
    "            self.label_encoders[col] = LabelEncoder()\n",
    "            self.df[col] = self.label_encoders[col].fit_transform(self.df[col])\n",
    "        \n",
    "        # Scale numeric features\n",
    "        self.scaler = StandardScaler()\n",
    "        self.tabular_features = self.scaler.fit_transform(self.df[tabular_cols])\n",
    "    \n",
    "    @staticmethod\n",
    "    def get_batch_embeddings(texts: list, tokenizer, model, device='cuda', batch_size=32) -> np.ndarray:\n",
    "        embeddings = []\n",
    "        \n",
    "        for i in tqdm(range(0, len(texts), batch_size), desc=\"Computing embeddings\"):\n",
    "            batch_texts = texts[i:i + batch_size]\n",
    "            encoded_input = tokenizer(batch_texts, \n",
    "                                    padding=True, \n",
    "                                    truncation=True, \n",
    "                                    max_length=512,\n",
    "                                    return_tensors='pt')\n",
    "            \n",
    "            # Move input to device\n",
    "            encoded_input = {k: v.to(device) for k, v in encoded_input.items()}\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                model_output = model(**encoded_input)\n",
    "                batch_embeddings = model_output[0][:, 0]  # CLS token embedding\n",
    "                batch_embeddings = torch.nn.functional.normalize(batch_embeddings, p=2, dim=1)\n",
    "                embeddings.append(batch_embeddings.cpu().numpy())\n",
    "        \n",
    "        return np.vstack(embeddings)\n",
    "    \n",
    "    def get_text_embeddings(self, texts: list) -> np.ndarray:\n",
    "        device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "        model = AutoModel.from_pretrained('BAAI/bge-small-en-v1.5')\n",
    "        model = model.to(device)\n",
    "        model.eval()\n",
    "        \n",
    "        return self.get_batch_embeddings(texts, self.tokenizer, model, device)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        tabular_features = torch.FloatTensor(self.tabular_features[idx])\n",
    "        text_embedding = torch.FloatTensor(self.text_embeddings[idx])\n",
    "        \n",
    "        if self.is_test:\n",
    "            return tabular_features, text_embedding\n",
    "        \n",
    "        label = torch.LongTensor([self.labels[idx]])[0]\n",
    "        return tabular_features, text_embedding, label\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training on chunk 1/1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing tabular features...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2a4244b080414c20af8d38be9d15f1e7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Encoding categorical columns:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing text embeddings...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0c81cf20f5754b1e97b1a81d7a34cc5a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Computing embeddings:   0%|          | 0/15625 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "97280926172d414ba212b91b6f23a7ef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Computing embeddings:   0%|          | 0/3125 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing tabular features...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8c4db17c31494824abf77b8a4564eaad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Encoding categorical columns:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33manindyamitra2018\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.2"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>./wandb/run-20250111_180644-aar075sk</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/anindyamitra2018/accident-severity-prediction/runs/aar075sk' target=\"_blank\">hybrid-model-chunk-0</a></strong> to <a href='https://wandb.ai/anindyamitra2018/accident-severity-prediction' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/anindyamitra2018/accident-severity-prediction' target=\"_blank\">https://wandb.ai/anindyamitra2018/accident-severity-prediction</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/anindyamitra2018/accident-severity-prediction/runs/aar075sk' target=\"_blank\">https://wandb.ai/anindyamitra2018/accident-severity-prediction/runs/aar075sk</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n",
      "\n",
      "  | Name            | Type             | Params | Mode \n",
      "-------------------------------------------------------------\n",
      "0 | tabular_network | Sequential       | 12.5 K | train\n",
      "1 | text_network    | Sequential       | 57.5 K | train\n",
      "2 | classifier      | Sequential       | 8.5 K  | train\n",
      "3 | criterion       | CrossEntropyLoss | 0      | train\n",
      "-------------------------------------------------------------\n",
      "78.5 K    Trainable params\n",
      "0         Non-trainable params\n",
      "78.5 K    Total params\n",
      "0.314     Total estimated model params size (MB)\n",
      "20        Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1bfc318b201e41cfb1b5e6f2d0196508",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "afefc2ac24d54936b262936e25dca174",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8617e479dd7f4ca7aa3016ef3399d3d2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "15212f893d63409b9213935aab9a976b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ac5ea7fe54a44737a0e4f6e8c9b0f281",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "472cab718ee949d9b5673df53e4bc678",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "607be9447bb3428c95500450a98ba2dc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "# Load your full data\n",
    "# train_df = pd.read_csv('/teamspace/studios/this_studio/Assignment-TechstaX/data/imputed_dataset.csv')  # (5796296, 34)\n",
    "# val_df = pd.read_csv('/teamspace/studios/this_studio/Assignment-TechstaX/data/val_data.csv')      # (1159259, 34)\n",
    "\n",
    "# # Split data into chunks and save\n",
    "# train_chunks, val_chunks = split_and_save_data(\n",
    "#     train_df, \n",
    "#     val_df,\n",
    "#     train_chunk_size=500000,\n",
    "#     val_chunk_size=100000,\n",
    "#     output_dir='/teamspace/studios/this_studio/Assignment-TechstaX/data/data_chunks'\n",
    "# )\n",
    "\n",
    "train_chunks = ['/teamspace/studios/this_studio/Assignment-TechstaX/data/data_chunks/train_chunk_0.csv']\n",
    "val_chunks = ['/teamspace/studios/this_studio/Assignment-TechstaX/data/data_chunks/val_chunk_0.csv']\n",
    "# Train model incrementally\n",
    "model, label_encoder = train_model_incrementally(\n",
    "    train_chunks,\n",
    "    val_chunks,\n",
    "    model_dir='/teamspace/studios/this_studio/Assignment-TechstaX/models',\n",
    "    batch_size=32,\n",
    "    epochs_per_chunk=10\n",
    ")\n",
    "\n",
    "# Save final model\n",
    "torch.save({\n",
    "    'model_state_dict': model.state_dict(),\n",
    "    'label_encoder': label_encoder\n",
    "}, '/teamspace/studios/this_studio/Assignment-TechstaX/models/final_model.pt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.listdir(\"/teamspace/studios/this_studio/Assignment-TechstaX/data/data_chunks\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
