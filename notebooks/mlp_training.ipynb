{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from sklearn.utils import shuffle\n",
    "from typing import List, Tuple\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import pytorch_lightning as pl\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint, EarlyStopping, TQDMProgressBar\n",
    "from pytorch_lightning.loggers import WandbLogger\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import wandb\n",
    "from sklearn.metrics import classification_report, accuracy_score, f1_score\n",
    "from typing import Dict, Tuple\n",
    "from tqdm.auto import tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class LitProgressBar(TQDMProgressBar):\n",
    "    def init_validation_tqdm(self):\n",
    "        bar = super().init_validation_tqdm()\n",
    "        bar.set_description('Validation')\n",
    "        return bar\n",
    "    \n",
    "    def init_train_tqdm(self):\n",
    "        bar = super().init_train_tqdm()\n",
    "        bar.set_description('Training')\n",
    "        return bar\n",
    "    \n",
    "    \n",
    "class AccidentPredictor(pl.LightningModule):\n",
    "    def __init__(self, \n",
    "                 tabular_dim: int,\n",
    "                 embedding_dim: int,\n",
    "                 num_classes: int,\n",
    "                 learning_rate: float = 1e-3):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "        \n",
    "        # Network architecture\n",
    "        self.tabular_network = nn.Sequential(\n",
    "            nn.Linear(tabular_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2)\n",
    "        )\n",
    "        \n",
    "        self.text_network = nn.Sequential(\n",
    "            nn.Linear(embedding_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2)\n",
    "        )\n",
    "        \n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(64, num_classes)\n",
    "        )\n",
    "        \n",
    "        self.criterion = nn.CrossEntropyLoss()\n",
    "        \n",
    "    def forward(self, tabular_features, text_embedding):\n",
    "        tabular_features = self.tabular_network(tabular_features)\n",
    "        text_features = self.text_network(text_embedding)\n",
    "        combined = torch.cat([tabular_features, text_features], dim=1)\n",
    "        return self.classifier(combined)\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        tabular_features, text_embedding, labels = batch\n",
    "        logits = self(tabular_features, text_embedding)\n",
    "        loss = self.criterion(logits, labels)\n",
    "        \n",
    "        preds = torch.argmax(logits, dim=1)\n",
    "        acc = accuracy_score(labels.cpu(), preds.cpu())\n",
    "        f1 = f1_score(labels.cpu(), preds.cpu(), average='weighted')\n",
    "        \n",
    "        self.log('train_loss', loss, prog_bar=True)\n",
    "        self.log('train_acc', acc, prog_bar=True)\n",
    "        self.log('train_f1', f1, prog_bar=True)\n",
    "        \n",
    "        return loss\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        tabular_features, text_embedding, labels = batch\n",
    "        logits = self(tabular_features, text_embedding)\n",
    "        loss = self.criterion(logits, labels)\n",
    "        \n",
    "        preds = torch.argmax(logits, dim=1)\n",
    "        acc = accuracy_score(labels.cpu(), preds.cpu())\n",
    "        f1 = f1_score(labels.cpu(), preds.cpu(), average='weighted')\n",
    "        \n",
    "        self.log('val_loss', loss, prog_bar=True)\n",
    "        self.log('val_acc', acc, prog_bar=True)\n",
    "        self.log('val_f1', f1, prog_bar=True)\n",
    "        \n",
    "        return loss\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.parameters(), \n",
    "                                   lr=self.hparams.learning_rate)\n",
    "        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "            optimizer, mode='min', factor=0.1, patience=5, verbose=True\n",
    "        )\n",
    "        return {\n",
    "            'optimizer': optimizer,\n",
    "            'lr_scheduler': scheduler,\n",
    "            'monitor': 'val_loss'\n",
    "        }\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Modified AccidentDataset class\n",
    "class AccidentDataset(Dataset):\n",
    "    def __init__(self, df: pd.DataFrame, tokenizer, text_embeddings=None, \n",
    "                 is_test: bool = False, label_encoder=None):\n",
    "        self.df = df\n",
    "        self.tokenizer = tokenizer\n",
    "        self.is_test = is_test\n",
    "        \n",
    "        # Process tabular features\n",
    "        print(\"Processing tabular features...\")\n",
    "        self.process_tabular_features()\n",
    "        \n",
    "        # Store or compute text embeddings\n",
    "        if text_embeddings is not None:\n",
    "            self.text_embeddings = text_embeddings\n",
    "        else:\n",
    "            print(\"Computing text embeddings...\")\n",
    "            self.text_embeddings = self.get_text_embeddings(df['Description'].tolist())\n",
    "        \n",
    "        if not is_test:\n",
    "            if label_encoder is None:\n",
    "                self.label_encoder = LabelEncoder()\n",
    "                self.labels = self.label_encoder.fit_transform(df['Severity'])\n",
    "            else:\n",
    "                self.label_encoder = label_encoder\n",
    "                self.labels = self.label_encoder.transform(df['Severity'])\n",
    "    \n",
    "    def process_tabular_features(self):\n",
    "        # Select all columns except 'Severity' and 'Description'\n",
    "        tabular_cols = [col for col in self.df.columns \n",
    "                       if col not in ['Severity', 'Description']]\n",
    "        \n",
    "        # Convert boolean columns to int\n",
    "        bool_cols = self.df[tabular_cols].select_dtypes(include=['bool']).columns\n",
    "        for col in bool_cols:\n",
    "            self.df[col] = self.df[col].astype(int)\n",
    "        \n",
    "        # Convert categorical columns to numeric using label encoding\n",
    "        cat_cols = self.df[tabular_cols].select_dtypes(include=['object']).columns\n",
    "        self.label_encoders = {}\n",
    "        for col in tqdm(cat_cols, desc=\"Encoding categorical columns\"):\n",
    "            self.label_encoders[col] = LabelEncoder()\n",
    "            self.df[col] = self.label_encoders[col].fit_transform(self.df[col])\n",
    "        \n",
    "        # Scale numeric features\n",
    "        self.scaler = StandardScaler()\n",
    "        self.tabular_features = self.scaler.fit_transform(self.df[tabular_cols])\n",
    "    \n",
    "    @staticmethod\n",
    "    def get_batch_embeddings(texts: list, tokenizer, model, device='cuda', batch_size=32) -> np.ndarray:\n",
    "        embeddings = []\n",
    "        \n",
    "        for i in tqdm(range(0, len(texts), batch_size), desc=\"Computing embeddings\"):\n",
    "            batch_texts = texts[i:i + batch_size]\n",
    "            encoded_input = tokenizer(batch_texts, \n",
    "                                    padding=True, \n",
    "                                    truncation=True, \n",
    "                                    max_length=512,\n",
    "                                    return_tensors='pt')\n",
    "            \n",
    "            # Move input to device\n",
    "            encoded_input = {k: v.to(device) for k, v in encoded_input.items()}\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                model_output = model(**encoded_input)\n",
    "                batch_embeddings = model_output[0][:, 0]  # CLS token embedding\n",
    "                batch_embeddings = torch.nn.functional.normalize(batch_embeddings, p=2, dim=1)\n",
    "                embeddings.append(batch_embeddings.cpu().numpy())\n",
    "        \n",
    "        return np.vstack(embeddings)\n",
    "    \n",
    "    def get_text_embeddings(self, texts: list) -> np.ndarray:\n",
    "        device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "        model = AutoModel.from_pretrained('BAAI/bge-small-en-v1.5')\n",
    "        model = model.to(device)\n",
    "        model.eval()\n",
    "        \n",
    "        return self.get_batch_embeddings(texts, self.tokenizer, model, device)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        tabular_features = torch.FloatTensor(self.tabular_features[idx])\n",
    "        text_embedding = torch.FloatTensor(self.text_embeddings[idx])\n",
    "        \n",
    "        if self.is_test:\n",
    "            return tabular_features, text_embedding\n",
    "        \n",
    "        label = torch.LongTensor([self.labels[idx]])[0]\n",
    "        return tabular_features, text_embedding, label\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import torch\n",
    "import pytorch_lightning as pl\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "from typing import List, Tuple\n",
    "import json\n",
    "from glob import glob\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from torch.utils.data import DataLoader\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint, EarlyStopping, TQDMProgressBar\n",
    "from pytorch_lightning.loggers import WandbLogger\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class MetricsLogger:\n",
    "    def __init__(self, log_dir: str):\n",
    "        self.log_dir = Path(log_dir)\n",
    "        self.log_dir.mkdir(parents=True, exist_ok=True)\n",
    "        self.timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "        self.metrics_file = self.log_dir / f\"training_metrics_remaining_chunks_{self.timestamp}.txt\"\n",
    "        self.summary_file = self.log_dir / f\"training_summary_remaining_chunks_{self.timestamp}.json\"\n",
    "        self.chunk_metrics = {}\n",
    "        \n",
    "    def log_chunk_start(self, chunk_idx: int):\n",
    "        with open(self.metrics_file, 'a') as f:\n",
    "            f.write(f\"\\n{'='*50}\\n\")\n",
    "            f.write(f\"Starting training on chunk {chunk_idx}\\n\")\n",
    "            f.write(f\"Timestamp: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\")\n",
    "            f.write(f\"{'='*50}\\n\")\n",
    "        \n",
    "        self.chunk_metrics[chunk_idx] = {\n",
    "            'epochs': [],\n",
    "            'best_val_loss': float('inf'),\n",
    "            'best_val_acc': 0,\n",
    "            'best_val_f1': 0\n",
    "        }\n",
    "    \n",
    "    def log_epoch_metrics(self, chunk_idx: int, epoch: int, metrics: dict):\n",
    "        epoch_log = (\n",
    "            f\"Epoch {epoch:02d} - \"\n",
    "            f\"Train Loss: {metrics['train_loss']:.4f} - \"\n",
    "            f\"Train Acc: {metrics['train_acc']:.4f} - \"\n",
    "            f\"Train F1: {metrics['train_f1']:.4f} - \"\n",
    "            f\"Val Loss: {metrics['val_loss']:.4f} - \"\n",
    "            f\"Val Acc: {metrics['val_acc']:.4f} - \"\n",
    "            f\"Val F1: {metrics['val_f1']:.4f}\\n\"\n",
    "        )\n",
    "        \n",
    "        with open(self.metrics_file, 'a') as f:\n",
    "            f.write(epoch_log)\n",
    "        \n",
    "        self.chunk_metrics[chunk_idx]['epochs'].append(metrics)\n",
    "        \n",
    "        # Update best metrics\n",
    "        if metrics['val_loss'] < self.chunk_metrics[chunk_idx]['best_val_loss']:\n",
    "            self.chunk_metrics[chunk_idx]['best_val_loss'] = metrics['val_loss']\n",
    "        if metrics['val_acc'] > self.chunk_metrics[chunk_idx]['best_val_acc']:\n",
    "            self.chunk_metrics[chunk_idx]['best_val_acc'] = metrics['val_acc']\n",
    "        if metrics['val_f1'] > self.chunk_metrics[chunk_idx]['best_val_f1']:\n",
    "            self.chunk_metrics[chunk_idx]['best_val_f1'] = metrics['val_f1']\n",
    "    \n",
    "    def save_summary(self):\n",
    "        summary = {\n",
    "            'training_start': self.timestamp,\n",
    "            'training_end': datetime.now().strftime(\"%Y%m%d_%H%M%S\"),\n",
    "            'chunk_metrics': self.chunk_metrics\n",
    "        }\n",
    "        \n",
    "        with open(self.summary_file, 'w') as f:\n",
    "            json.dump(summary, f, indent=4)\n",
    "\n",
    "class MetricsCallback(pl.Callback):\n",
    "    def __init__(self, metrics_logger, chunk_idx):\n",
    "        self.metrics_logger = metrics_logger\n",
    "        self.chunk_idx = chunk_idx\n",
    "    \n",
    "    def on_train_epoch_end(self, trainer, pl_module):\n",
    "        metrics = {\n",
    "            'train_loss': trainer.callback_metrics['train_loss'].item(),\n",
    "            'train_acc': trainer.callback_metrics['train_acc'].item(),\n",
    "            'train_f1': trainer.callback_metrics['train_f1'].item(),\n",
    "            'val_loss': trainer.callback_metrics['val_loss'].item(),\n",
    "            'val_acc': trainer.callback_metrics['val_acc'].item(),\n",
    "            'val_f1': trainer.callback_metrics['val_f1'].item()\n",
    "        }\n",
    "        self.metrics_logger.log_epoch_metrics(self.chunk_idx, trainer.current_epoch, metrics)\n",
    "\n",
    "def get_remaining_chunks(data_dir: str):\n",
    "    \"\"\"Get all remaining chunks except chunk_0\"\"\"\n",
    "    train_chunks = sorted(glob(os.path.join(data_dir, 'train_chunk_*.csv')))\n",
    "    val_chunks = sorted(glob(os.path.join(data_dir, 'val_chunk_*.csv')))\n",
    "    \n",
    "    # Remove chunk_0 files\n",
    "    train_chunks = [f for f in train_chunks if not f.endswith('_0.csv')]\n",
    "    val_chunks = [f for f in val_chunks if not f.endswith('_0.csv')]\n",
    "    \n",
    "    return train_chunks, val_chunks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train_remaining_chunks(\n",
    "    data_dir: str,\n",
    "    model_dir: str = 'models',\n",
    "    log_dir: str = 'logs',\n",
    "    batch_size: int = 32,\n",
    "    epochs_per_chunk: int = 10,\n",
    "    project_name: str = 'accident-severity-prediction'\n",
    "):\n",
    "    \"\"\"\n",
    "    Train the model on remaining chunks, starting from the saved state of chunk_0\n",
    "    \"\"\"\n",
    "    # Get remaining chunks\n",
    "    train_chunks, val_chunks = get_remaining_chunks(data_dir)\n",
    "    print(f\"Found {len(train_chunks)} remaining train chunks and {len(val_chunks)} remaining val chunks\")\n",
    "    \n",
    "    # Load the model from chunk_0\n",
    "    checkpoint_path = f'{model_dir}/model_after_chunk_0.pt'\n",
    "    if not os.path.exists(checkpoint_path):\n",
    "        raise FileNotFoundError(f\"Couldn't find checkpoint from chunk_0 at {checkpoint_path}\")\n",
    "    \n",
    "    checkpoint = torch.load(checkpoint_path)\n",
    "    \n",
    "    # Initialize metrics logger\n",
    "    metrics_logger = MetricsLogger(log_dir)\n",
    "    tokenizer = AutoTokenizer.from_pretrained('BAAI/bge-small-en-v1.5')\n",
    "    \n",
    "    # Load first chunk to get dimensions\n",
    "    temp_dataset = AccidentDataset(pd.read_csv(train_chunks[0]), tokenizer)\n",
    "    \n",
    "    # Initialize model with saved weights\n",
    "    model = AccidentPredictor(\n",
    "        tabular_dim=temp_dataset.tabular_features.shape[1],\n",
    "        embedding_dim=temp_dataset.text_embeddings.shape[1],\n",
    "        num_classes=len(temp_dataset.label_encoder.classes_)\n",
    "    )\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    label_encoder = checkpoint['label_encoder']\n",
    "    \n",
    "    del temp_dataset\n",
    "    \n",
    "    # Train on remaining chunks\n",
    "    for chunk_idx, (train_chunk, val_chunk) in enumerate(zip(train_chunks, val_chunks), start=1):\n",
    "        # Get actual chunk number from filename\n",
    "        actual_chunk_num = int(train_chunk.split('_')[-1].split('.')[0])\n",
    "        metrics_logger.log_chunk_start(actual_chunk_num)\n",
    "        print(f\"\\nTraining on chunk {actual_chunk_num} ({chunk_idx}/{len(train_chunks)})\")\n",
    "        \n",
    "        # Load data chunks\n",
    "        train_df = pd.read_csv(train_chunk)\n",
    "        val_df = pd.read_csv(val_chunk)\n",
    "        \n",
    "        # Create datasets\n",
    "        train_dataset = AccidentDataset(train_df, tokenizer, label_encoder=label_encoder)\n",
    "        val_dataset = AccidentDataset(\n",
    "            val_df, \n",
    "            tokenizer,\n",
    "            text_embeddings=train_dataset.get_text_embeddings(val_df['Description'].tolist()),\n",
    "            label_encoder=label_encoder\n",
    "        )\n",
    "        \n",
    "        # Create dataloaders\n",
    "        train_loader = DataLoader(\n",
    "            train_dataset, \n",
    "            batch_size=batch_size, \n",
    "            shuffle=True, \n",
    "            num_workers=4,\n",
    "            pin_memory=True\n",
    "        )\n",
    "        val_loader = DataLoader(\n",
    "            val_dataset, \n",
    "            batch_size=batch_size, \n",
    "            shuffle=False, \n",
    "            num_workers=4,\n",
    "            pin_memory=True\n",
    "        )\n",
    "        \n",
    "        # Initialize wandb logger\n",
    "        wandb_logger = WandbLogger(\n",
    "            project=project_name,\n",
    "            name=f'hybrid-model-chunk-{actual_chunk_num}',\n",
    "            log_model=True\n",
    "        )\n",
    "        \n",
    "        # Initialize trainer\n",
    "        trainer = pl.Trainer(\n",
    "            max_epochs=epochs_per_chunk,\n",
    "            logger=wandb_logger,\n",
    "            callbacks=[\n",
    "                ModelCheckpoint(\n",
    "                    dirpath=f'{model_dir}/chunk_{actual_chunk_num}',\n",
    "                    filename='accident-predictor-{epoch:02d}-{val_loss:.2f}',\n",
    "                    save_top_k=1,\n",
    "                    mode='min'\n",
    "                ),\n",
    "                EarlyStopping(\n",
    "                    monitor='val_loss',\n",
    "                    patience=5,\n",
    "                    mode='min'\n",
    "                ),\n",
    "                LitProgressBar(),\n",
    "                MetricsCallback(metrics_logger, actual_chunk_num)\n",
    "            ],\n",
    "            accelerator='auto',\n",
    "            devices=1,\n",
    "            log_every_n_steps=10\n",
    "        )\n",
    "        \n",
    "        # Train model\n",
    "        trainer.fit(model, train_loader, val_loader)\n",
    "        \n",
    "        # Save model after each chunk\n",
    "        chunk_save_path = f'{model_dir}/model_after_chunk_{actual_chunk_num}.pt'\n",
    "        torch.save({\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'label_encoder': label_encoder\n",
    "        }, chunk_save_path)\n",
    "        \n",
    "        # Clear memory\n",
    "        del train_dataset, val_dataset, train_loader, val_loader\n",
    "        torch.cuda.empty_cache()\n",
    "    \n",
    "    # Save final model and training summary\n",
    "    final_save_path = f'{model_dir}/final_model.pt'\n",
    "    torch.save({\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'label_encoder': label_encoder\n",
    "    }, final_save_path)\n",
    "    \n",
    "    metrics_logger.save_summary()\n",
    "    return model, label_encoder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    # Define paths\n",
    "    data_dir = '/teamspace/studios/this_studio/Assignment-TechstaX/data/data_chunks'\n",
    "    model_dir = '/teamspace/studios/this_studio/Assignment-TechstaX/models'\n",
    "    log_dir = '/teamspace/studios/this_studio/Assignment-TechstaX/logs'\n",
    "    \n",
    "    # Train on remaining chunks\n",
    "    model, label_encoder = train_remaining_chunks(\n",
    "        data_dir=data_dir,\n",
    "        model_dir=model_dir,\n",
    "        log_dir=log_dir,\n",
    "        batch_size=32,\n",
    "        epochs_per_chunk=10,\n",
    "        project_name='accident-severity-prediction'\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
